{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Machine Learning & Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a) Adam Optimizer\n",
    "Recall the standard Stochastic Gradient Descent update rule:__\n",
    "\n",
    "$$\\theta \\leftarrow \\theta - \\alpha \\Delta_\\theta J_{\\text {minibatch}}(\\theta)$$\n",
    "\n",
    "__where $\\theta$ is a vector containing all of the model parameters, $J$ is the loss function, $J_{\\text {minibatch}}(\\theta)$ is the gradient of the loss function with respect to the parameters on a minibatch of data, and $\\alpha$ is the learning rate. Adam Optimization uses a more sophisticated update rule with two additional\n",
    "steps:__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__i. First, Adam uses a trick called momentum by keeping track of m, a rolling average of the gradients:__\n",
    "\n",
    "$$m \\leftarrow \\beta_1 m + (1 - \\beta_1) \\Delta_\\theta J_{\\text {minibatch}}(\\theta) \\\\\n",
    "\\theta \\leftarrow \\theta - \\alpha m$$\n",
    "\n",
    "__where $\\beta_1$ is a hyperparameter between 0 and 1 (often set to 0.9). Briefly explain (you don’t need\n",
    "to prove mathematically, just give an intuition) how using m stops the updates from varying as much and why this low variance may be helpful to learning, overall.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "On each iteration in vanilla SGD we are making a step in gradient's direction. Due usage of minibatches these steps can vary a lot and path to optimum can look like a zigzag. When applying more complex update rule incorporating rolling averages of the gradients, we smooth our path. Every step is a sum of two vectors - big fraction of the previous and a small fraction of the new one. Overall it makes SGD to perform less wrong steps and it converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ii. Adam also uses adaptive learning rates by keeping track of v, a rolling average of the magnitudes of the gradients:__\n",
    "\n",
    "$$m \\leftarrow \\beta_1 m + (1 - \\beta_1) \\Delta_\\theta J_{\\text {minibatch}}(\\theta) \\\\\n",
    "v \\leftarrow \\beta_2 v + (1 - \\beta_2) (\\Delta_\\theta J_{\\text {minibatch}}(\\theta) \\odot \\Delta_\\theta J_{\\text {minibatch}}(\\theta)) \\\\\n",
    "\\theta \\leftarrow \\theta - \\alpha \\odot m / \\sqrt {v}$$\n",
    "\n",
    "__where $\\odot$ and $/$ denote elementwise multiplication and division (so $z \\odot z$ is elementwise squaring) and $\\beta_2$ is a hyperparameter between 0 and 1 (often set to 0.99). Since Adam divides the update by $\\sqrt {v}$, which of the model parameters will get larger updates? Why might this help with learning?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "Weights which showed stability on a number of iterations (previous $v$ and $m$ are close to new $v$ and $m$) will receive larger updates than those which fluctuate too much between iterations. This works like a filter - we prefer to believe in a stable gradients rather than random caused by minibatch nature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b) Dropout is a regularization technique. During training, dropout randomly sets units in the hidden layer $h$ to zero with probability $p_{\\text drop}$ (dropping different units each minibatch), and then multiplies $h$ by a constant $\\gamma$. We can write this as__\n",
    "\n",
    "$$h_{\\text drop} = \\gamma d \\circ h$$\n",
    "\n",
    "__where $d \\in \\{0,1\\}^{D_h}$ ($D_h$ is the size of $h$) is a mask vector where each entry is 0 with probability $p_{\\text drop}$ and 1 with probability $(1 − p_{\\text drop})$. $\\gamma$ is chosen such that the expected value of $h_{\\text drop}$ is $h$:__\n",
    "\n",
    "$$E_{p_{\\text drop}}[h_{\\text drop}]_i = h_i$$\n",
    "\n",
    "__for all $i \\in \\{1, \\ldots, D_h\\}$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__i. What must $\\gamma$ equal in terms of $p_{\\text drop}$? Briefly justify your answer.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "$\\gamma$ should be equal to $p_{\\text drop}$. This makes flowing values and gradients scale back to the original size like if there was no dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ii. Why should we apply dropout during training but not during evaluation?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "we don't want to remember all vectors $d$ to repeat our process of dropout in test time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Transition-Based Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a) Go through the sequence of transitions needed for parsing the sentence “I parsed this sentence correctly”. The dependency tree for the sentence is shown below. At each step, give the configuration of the stack and buffer, as well as what transition was applied this step and what new dependency was added (if any). The first three steps are provided below as an example.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Stack | Buffer | New Dependency | Transition |\n",
    "| - | - | - | - |\n",
    "| ROOT  | I parsed this sentence correctly | - | Init |\n",
    "| ROOT I | parsed this sentence correctly | -| Shift |\n",
    "| ROOT I parsed | this sentence correctly | - | Shift |\n",
    "| ROOT parsed | this sentence correctly | parsed $\\rightarrow$ I | Left-Arc |\n",
    "| ROOT parsed this | sentence correctly | - | Shift |\n",
    "| ROOT parsed this sentence | correctly | - | Shift |\n",
    "| ROOT parsed sentence | correctly | sentence $\\rightarrow$ this | Left-Arc |\n",
    "| ROOT parsed | correctly | parsed $\\rightarrow$ sentence | Right-Arc |\n",
    "| ROOT parsed correctly | $\\varnothing$ | - | Shift |\n",
    "| ROOT parsed | $\\varnothing$ | parsed $\\rightarrow$ correctly | Right-Arc |\n",
    "| ROOT | $\\varnothing$ | ROOT $\\rightarrow$ parsed | Right-Arc |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b) A sentence containing $n$ words will be parsed in how many steps (in terms of $n$)? Briefly\n",
    "explain why.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "Each word can be moved in two ways: from \"buffer\" to \"stack\" during \"Shift\" transition and from \"Stack\" to \"Dependencies\" during \"Left-Arc\" or \"Right-Arc\" transition. Every movement can be performed only once for every word - algorithm doesn't allow to return word back to any list. So we will perform 2 movements for every word, i.e. $O(2*n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__f) In this question are four sentences with dependency parses obtained from a parser. Each sentence\n",
    "has one error, and there is one example of each of the four types above. For each sentence, state\n",
    "the type of error, the incorrect dependency, and the correct dependency.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "i.\n",
    "\n",
    "Error type: Verb Phrase Attachment Error\n",
    "\n",
    "Incorrect dependency: wedding $\\rightarrow$ fearing\n",
    "\n",
    "Correct dependency: heading $\\rightarrow$ fearing\n",
    "\n",
    "ii.\n",
    "\n",
    "Error type: Coordination Attachment Error\n",
    "\n",
    "Incorrect dependency: makes $\\rightarrow$ rescue\n",
    "\n",
    "Correct dependency: rush $\\rightarrow$ rescue\n",
    "\n",
    "iii.\n",
    "\n",
    "Error type: Prepositional Phrase Attachment Error\n",
    "\n",
    "Incorrect dependency: named $\\rightarrow$ Midland\n",
    "\n",
    "Correct dependency: guy $\\rightarrow$ Midland\n",
    "\n",
    "iv.\n",
    "\n",
    "Error type: Modifier Attachment Error\n",
    "\n",
    "Incorrect dependency: elements $\\rightarrow$ most\n",
    "\n",
    "Correct dependency: crucial $\\rightarrow$ most"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of model evaluation on a test set:\n",
    "```\n",
    "Final evaluation on test set\n",
    "2919736it [00:00, 57730250.86it/s]                        \n",
    "- test UAS: 89.04\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
