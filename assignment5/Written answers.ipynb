{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU score on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Corpus BLEU: 24.179620034653208\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-based convolutional encoder for NMT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) In Assignment 4 we used 256-dimensional word embeddings ($e_{word} = 256$),\n",
    "while in this assignment, it turns out that a character embedding size of 50 suffices ($e_{char} = 50$).\n",
    "In 1-2 sentences, explain one reason why the embedding size used for character-level embeddings is\n",
    "typically lower than that used for word embeddings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Number of characters, its combinations and coocurrences is much lower than this number for words in corpus. That's the reason why we don't need too big vectors to represent characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Write down the total number of parameters in the character-based embedding model (Figure 2), then do the same for the word-based lookup embedding model (Figure 1). Write each answer as a single expression (though you may show working) in terms of $e_{char}$, $k$, $e_{word}$, $V_{word}$ (the size of the word-vocabulary in the lookup embedding model) and $V_{char}$ (the size of the character-vocabulary in the character-based embedding model). Given that in our code, $k = 5$, $V_{word} ≈ 50000$ and $V_char = 96$, state which model has more parameters, and by what factor (e.g. twice as many? a thousand times as many?).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "For character-based embedding volume we parameterize 3 parts: embedding layer, convolutional layer and highway layer. So there will be 3 parts in our sum:\n",
    "\n",
    "$\\text{Total parameters}_\\text{char_based} = V_{char}*l_{char} + l_{word}*l_{char}*k + 2 * (l_{word}*l_{word} + l_{word}) \\stackrel{\\text{with values}}{=} 96*50 + 256*50*5 + 2*(256*256 + 256) = 200384$\n",
    "\n",
    "For word-based embedding there is only one embedding layer: \n",
    "\n",
    "$\\text{Total parameters}_\\text{word_based} = V_{word}*l_{word} \\stackrel{\\text{with values}}{=} 50000*256 = 1280000$\n",
    "\n",
    "So character-based model has more than 6 times less parameters parameters than word-based.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) In step 3 of the character-based embedding model, instead of using a 1D con-\n",
    "vnet, we could have used a RNN instead (e.g. feed the sequence of characters into a bidirectional\n",
    "LSTM and combine the hidden states using max-pooling). Explain one advantage of using a con-\n",
    "volutional architecture rather than a recurrent architecture for this purpose, making it clear how\n",
    "the two contrast.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "We showed above that number of parameters in 1D-CNN (particularly for Highway network) is $2 * (l_{word}*l_{word} + l_{word})$. Number of parameters for BiLSTM is $2*(4*(l_{char}+1)*l_{word} + l_{word}^2)$ and we can see that this number is bigger than number for 1D-CNN due <$4*(l_{char}+1)$> term. So we can say that using BiLSTM instead of 1D-CNN can be less effective in terms of number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) In lectures we learned about both max-pooling and average-pooling. For each\n",
    "pooling method, please explain one advantage in comparison to the other pooling method. For\n",
    "each advantage, make it clear how the two contrast, and write to a similar level of detail as in the\n",
    "example given in the previous question.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Average pooling takes in account every undelying feature in feature map, so if we don't want to pay maximum attention to only one feature than we should use average pooling. However if mean value of features in feature map is close to 0 than using average pooling will produce feature with value close to 0, like if this feature is not active at all.\n",
    "Max pooling is good when we want to pay all attention to the most active feature in a feature map. This makes this pooling more suitable for sparse features (when there are only few feeatures with big values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing NMT Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) The following table shows some of the forms of the Spanish word _traducir_, which means ‘to translate’.**\n",
    "\n",
    "| Form        | Spanish   | English     |\n",
    "| -           | -         | -           |\n",
    "| Infinitive  | traducir  | to translate |\n",
    "| Present     | traduzco  | I translate |\n",
    "| -           | traduces  | you translate |\n",
    "| -           | traduce   | he or she translates |\n",
    "| Subjunctive | traduzca  | that I translate |\n",
    "| -           | traduzcas | that you translate |\n",
    "\n",
    "**Use vocab.json to find (e.g. using grep) which of these six forms are in the word-vocabulary,\n",
    "which consists of the 50,000 most frequent words in the training data for English and for Spanish.\n",
    "Superstrings don’t count (e.g. having traducen in the vocabulary is not a hit for traduce). State\n",
    "which of these six forms occur, and which do not. Explain in one sentence why this is a bad thing for\n",
    "word-based NMT from Spanish to English. Then explain in detail (approximately two sentences)\n",
    "how our new character-aware NMT model may overcome this problem.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "There are no occurences of words \"traduzco\", \"traduces\", \"traduzca\" and \"traduzcas\" in top-50000 words of Spanish corpora. All this words will be replaced with special <UNK> token in word-based NMT and will not add any semantic meaning to the result. This will lead to poor performance for sentences with these words.\n",
    "\n",
    "Instead character-based NMT will try to produce some word vector for unknown words using character-based embedding. We hope, that our embedding will learn usage of suffixes in Spanish and will be able to connect these unknown words with the english word \"translate\" in a proper form.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.i) In Assignments 1 and 2, we investigated word embeddings created via algorithms such a Word2Vec, and found that for these embeddings, semantically similar words are close together in the embedding space. In this exercise, we’ll compare this with the word embeddings constructed using the CharCNN trained in our NMT system.\n",
    "Go to https://projector.tensorflow.org/. The website by default shows data from Word2Vec. Look at the nearest neighbors of the following words (in cosine distance).**\n",
    "\n",
    "- financial\n",
    "- neuron\n",
    "- Francisco\n",
    "- naturally\n",
    "- expectation\n",
    "\n",
    "**For each word, report the single closest neighbor. For your convenience, for each example take a screenshot of all the nearest words (so you can compare with the CharCNN embeddings).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "- financial: economic\n",
    "- neuron: nerve\n",
    "- Francisco: san\n",
    "- naturally: occuring\n",
    "- expectation: norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.ii) The TensorFlow embedding projector also allows you to upload your own data – you may find this useful in your projects!\n",
    "Now look at the nearest neighbors of the same words. Again, report the single closest neighbors and take screenshots for yourself.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "- financial: vertical\n",
    "- neuron: Newton\n",
    "- Francisco: France\n",
    "- naturally: practically\n",
    "- expectation: exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.iii) Compare the closest neighbors found by the two methods. Briefly describe what kind of similarity is modeled by Word2Vec. Briefly describe what kind of similarity is modeled by the CharCNN. Explain in detail (2-3 sentences) how the differences in the methodology of Word2Vec and a CharCNN explain the differences you have found.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "Word2Vec is based on cooccurences of words so 2 vectors will be close if their words are close in terms of meaning (they can be find in the same context). CharCNN is based on coocurences of characters instead. So 2 vectors will be close if their words consist of the similar set of characters in the simialr order: the same roots, same suffixes etc. And we can easily see this difference in our results - all the closest words for CharCNN are not similar to orifinal word in terms of meaning, but very close in terms of characters.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
