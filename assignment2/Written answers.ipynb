{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a) Show that the naive-softmax loss given in $J_{\\text {naive_softmax}}(v_c, o, U) = - \\log P(O=o|C=c)$ is the same as the cross-entropy loss between $y$ and $\\hat{y}$; i.e., show that:__\n",
    "\n",
    "$$- \\sum \\limits_{w \\in \\text {Vocab}} {y_w \\log \\hat {y}_w} = - \\log \\hat {y}_o$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "$y_w$ is non zero and equal to 1 only in the position $k$ of true word $o$. So left part can be simplified and be written as the only non zero term of a sum: $- \\sum \\limits_{w \\in \\text {Vocab}} {y_w \\log \\hat {y}_w} = - \\sum \\limits_{w = o} {y_w \\log \\hat {y}_w} = - y_o \\log \\hat{y}_o = - 1 * \\log \\hat{y}_o = - \\log \\hat{y}_o$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b) Compute the partial derivative of $J_{\\text {naive-softmax}}(v_c, o, U)$ with respect to $v_c$ . Please write your answer in terms of $y$, $\\hat{y}$, and $U$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "$\\frac {\\partial J} {\\partial v_c} = (y - \\hat{y})U^T$\n",
    "\n",
    "Numerical check (loss should be decreasing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: 1.5890904292435637 v_c [0.45885213 0.73172628 0.24022176 0.73334472]\n",
      "gradient [-0.26301605  0.01924709 -0.24670471  0.2366701 ]\n",
      "gradient check: [-2.33588093e-07 -3.83483198e-07 -2.60442183e-07 -1.53415490e-07]\n",
      "iteration 1\n",
      "Loss: 1.4083963494249763 v_c [0.72186817 0.71247919 0.48692647 0.49667461]\n",
      "gradient [-0.24973468  0.0107495  -0.23143907  0.21901466]\n",
      "gradient check: [-2.38928241e-07 -3.80015229e-07 -2.56052697e-07 -1.53460447e-07]\n",
      "iteration 2\n",
      "Loss: 1.2497108363431604 v_c [0.97160285 0.70172969 0.71836554 0.27765996]\n",
      "gradient [-0.23580589  0.00366018 -0.21633927  0.20207384]\n",
      "gradient check: [-2.42400880e-07 -3.72014821e-07 -2.50647304e-07 -1.51925110e-07]\n",
      "iteration 3\n",
      "Loss: 1.1113080418408678 v_c [1.20740874 0.69806951 0.9347048  0.07558611]\n",
      "gradient [-0.22167337 -0.0020538  -0.20163608  0.18606347]\n",
      "gradient check: [-2.43786463e-07 -3.60542837e-07 -2.44207683e-07 -1.49041492e-07]\n",
      "iteration 4\n",
      "Loss: 0.9912210325017193 v_c [ 1.42908211  0.70012332  1.13634089 -0.11047736]\n",
      "gradient [-0.20771652 -0.00651407 -0.18754095  0.17113717]\n",
      "gradient check: [-2.43094764e-07 -3.46738472e-07 -2.36828219e-07 -1.45019582e-07]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def J(y, U, v_c):\n",
    "    #softmax value for each word in vocabulary. Array shape [1,V], where V - size of vocabulary\n",
    "    y_hat = np.exp(U.T @ v_c) / np.sum(np.exp(U.T @ v_c))\n",
    "    #loss function. Scalar value.\n",
    "    J = np.sum(- y * np.log(y_hat))\n",
    "    return J\n",
    "\n",
    "def grad_v_c(y, U, v_c):\n",
    "    y_hat = np.exp(U.T @ v_c) / np.sum(np.exp(U.T @ v_c))\n",
    "    #sum over rows. so we get shape [1,d], where d is the size of our vector representation\n",
    "    d = np.sum((y_hat-y)[:,np.newaxis]*U.T, axis=0)\n",
    "    return d\n",
    "\n",
    "#some random matrix U (suppose d=4, v=5, so shape is 4x5)\n",
    "U = np.array(np.random.random((4,5)))\n",
    "\n",
    "#some random word vector V_c\n",
    "v_c = np.random.random(4)\n",
    "\n",
    "#some random vector y with correct labels of words from vocabulary (size v, one-hot encoded)\n",
    "y = np.array([0,0,1,0,0])\n",
    "\n",
    "#matrix with small shift for every element in word_vector v_c to perform finite differences check\n",
    "h = np.diag([0.00001]*4)\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(y,U,v_c)\n",
    "    dj = grad_v_c(y,U,v_c)\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j,\"v_c\",v_c)\n",
    "    print(\"gradient\",dj)\n",
    "    print(\"gradient check:\", dj - [(J(y,U,v_c+h[0])-j)/np.sum(h[0]),\n",
    "                                   (J(y,U,v_c+h[1])-j)/np.sum(h[1]),\n",
    "                                   (J(y,U,v_c+h[2])-j)/np.sum(h[2]),\n",
    "                                   (J(y,U,v_c+h[3])-j)/np.sum(h[3])])\n",
    "    v_c = v_c - dj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c) Compute the partial derivatives of $J_{\\text naive-softmax} (v_c, o, U)$ with respect to each of the ‘outside’\n",
    "word vectors, $u_w$ ’s. There will be two cases: when $w = o$, the true ‘outside’ word vector, and $w \\ne o$, for\n",
    "all other words. Please write you answer in terms of $y$, $\\hat{y}$, and $v_c$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$$\\frac {\\partial J} {\\partial U} = v_c (y - \\hat{y})^T$$ \n",
    "\n",
    "Numerical check (loss should be decreasing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: 2.0075040558004953 v_c [0.68634028 0.99313711 0.07836505 0.11857169]\n",
      "gradient [[ 0.12666534  0.11158217 -0.59414863  0.17204079  0.18386034]\n",
      " [ 0.18328524  0.16145984 -0.85973543  0.24894371  0.26604664]\n",
      " [ 0.01446241  0.01274024 -0.06783878  0.0196433   0.02099283]\n",
      " [ 0.02188262  0.01927686 -0.10264472  0.02972165  0.03176359]]\n",
      "gradient check: [[-3.54426763e-07 -3.20656466e-07 -2.73864997e-07 -4.42397011e-07\n",
      "  -4.61927854e-07]\n",
      " [-7.42177257e-07 -6.71423551e-07 -5.73425745e-07 -9.26322839e-07\n",
      "  -9.67190941e-07]\n",
      " [-4.59959692e-09 -4.15287579e-09 -3.55509641e-09 -5.77902785e-09\n",
      "  -6.02744712e-09]\n",
      " [-1.05645871e-08 -9.53816363e-09 -8.16228823e-09 -1.31796270e-08\n",
      "  -1.37556228e-08]]\n",
      "iteration 1\n",
      "Loss: 0.827735750645404 v_c [0.68634028 0.99313711 0.07836505 0.11857169]\n",
      "gradient [[ 0.08731442  0.07945574 -0.38638368  0.10755608  0.11205744]\n",
      " [ 0.12634431  0.11497277 -0.55909872  0.15563408  0.16214756]\n",
      " [ 0.0099694   0.00907211 -0.04411657  0.01228055  0.01279451]\n",
      " [ 0.01508438  0.01372672 -0.06675139  0.01858132  0.01935897]]\n",
      "gradient check: [[-2.61520872e-07 -2.41099003e-07 -5.79492905e-07 -3.11263674e-07\n",
      "  -3.21762689e-07]\n",
      " [-5.47581663e-07 -5.04826572e-07 -1.21334952e-06 -6.51723287e-07\n",
      "  -6.73724072e-07]\n",
      " [-3.41167951e-09 -3.14451641e-09 -7.57393681e-09 -4.06630707e-09\n",
      "  -4.19675861e-09]\n",
      " [-7.79807338e-09 -7.19416990e-09 -1.72981159e-08 -9.29405701e-09\n",
      "  -9.60796744e-09]]\n",
      "iteration 2\n",
      "Loss: 0.3741212651440623 v_c [0.68634028 0.99313711 0.07836505 0.11857169]\n",
      "gradient [[ 0.0495673   0.04587564 -0.21421127  0.05845463  0.0603137 ]\n",
      " [ 0.07172407  0.06638224 -0.30996456  0.08458408  0.08727417]\n",
      " [ 0.0056595   0.005238   -0.02445824  0.00667424  0.00688651]\n",
      " [ 0.00856321  0.00792545 -0.037007    0.01009858  0.01041976]]\n",
      "gradient check: [[-1.57802326e-07 -1.46925017e-07 -5.05670250e-07 -1.83523419e-07\n",
      "  -1.88788474e-07]\n",
      " [-3.30436186e-07 -3.07585163e-07 -1.05878090e-06 -3.84250484e-07\n",
      "  -3.95277523e-07]\n",
      " [-2.06594259e-09 -1.91020046e-09 -6.57824491e-09 -2.40354277e-09\n",
      "  -2.46861820e-09]\n",
      " [-4.70167223e-09 -4.38044789e-09 -1.50898428e-08 -5.49152088e-09\n",
      "  -5.62495501e-09]]\n",
      "iteration 3\n",
      "Loss: 0.22680591021664084 v_c [0.68634028 0.99313711 0.07836505 0.11857169]\n",
      "gradient [[ 0.03254983  0.03036597 -0.13927528  0.0376585   0.03870098]\n",
      " [ 0.04709973  0.04393968 -0.20153188  0.054492    0.05600047]\n",
      " [ 0.00371648  0.00346713 -0.01590219  0.00429978  0.00441881]\n",
      " [ 0.00562329  0.00524601 -0.0240611   0.00650586  0.00668595]]\n",
      "gradient check: [[-1.06387007e-07 -9.95846669e-08 -3.80954131e-07 -1.22140247e-07\n",
      "  -1.25319286e-07]\n",
      " [-2.22783524e-07 -2.08518152e-07 -7.97661589e-07 -2.55749244e-07\n",
      "  -2.62395505e-07]\n",
      " [-1.37010560e-09 -1.29761412e-09 -4.96580205e-09 -1.58576860e-09\n",
      "  -1.63869082e-09]\n",
      " [-3.16053896e-09 -2.96613572e-09 -1.13792319e-08 -3.63489713e-09\n",
      "  -3.72264097e-09]]\n",
      "iteration 4\n",
      "Loss: 0.16116987656567547 v_c [0.68634028 0.99313711 0.07836505 0.11857169]\n",
      "gradient [[ 0.0240106   0.02250522 -0.10216349  0.0274752   0.02817248]\n",
      " [ 0.03474343  0.03256514 -0.14783097  0.03975672  0.04076568]\n",
      " [ 0.00274149  0.0025696  -0.01166484  0.00313707  0.00321668]\n",
      " [ 0.00414805  0.00388799 -0.0176497   0.0047466   0.00486706]]\n",
      "gradient check: [[-7.95210059e-08 -7.47002484e-08 -2.98412007e-07 -9.05138121e-08\n",
      "  -9.27165102e-08]\n",
      " [-1.66485778e-07 -1.56404911e-07 -6.24808474e-07 -1.89523607e-07\n",
      "  -1.94109497e-07]\n",
      " [-1.03521991e-09 -9.84101316e-10 -3.89780670e-09 -1.18740871e-09\n",
      "  -1.20548270e-09]\n",
      " [-2.37354303e-09 -2.23965324e-09 -8.90770989e-09 -2.71393841e-09\n",
      "  -2.77494918e-09]]\n"
     ]
    }
   ],
   "source": [
    "def grad_U(y, U, v_c):\n",
    "    y_hat = np.exp(U.T @ v_c) / np.sum(np.exp(U.T @ v_c))\n",
    "    #sum over rows. so we get shape [1,d], where d is the size of our vector representation\n",
    "    d = v_c.T[:,np.newaxis] @ (y_hat-y).T[np.newaxis,:]\n",
    "    return d\n",
    "\n",
    "#some random matrix U (suppose d=4, v=5, so shape is 4x5)\n",
    "U = np.array(np.random.random((4,5)))\n",
    "\n",
    "#some random word vector V_c\n",
    "v_c = np.random.random(4)\n",
    "\n",
    "#some random vector y with correct labels of words from vocabulary (size v, one-hot encoded)\n",
    "y = np.array([0,0,1,0,0])\n",
    "\n",
    "#matrix with small shift for every element in word_vector v_c to perform finite differences check\n",
    "h = np.zeros((20,4,5))\n",
    "for i in range(20):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    h[i,row,col] = 0.00001\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(y,U,v_c)\n",
    "    dj = grad_U(y,U,v_c)\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j,\"v_c\",v_c)\n",
    "    print(\"gradient\",dj)\n",
    "    diff = np.zeros_like(U)\n",
    "    for i in range(20):\n",
    "        row = i // 5\n",
    "        col = i % 5\n",
    "        diff[row,col] = (J(y,U+h[i],v_c)-j)/np.sum(h[i])\n",
    "    print(\"gradient check:\", dj - diff)\n",
    "    U = U - dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
