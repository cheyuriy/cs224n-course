{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a) Show that the naive-softmax loss given in $J_{\\text {naive_softmax}}(v_c, o, U) = - \\log P(O=o|C=c)$ is the same as the cross-entropy loss between $y$ and $\\hat{y}$; i.e., show that:__\n",
    "\n",
    "$$- \\sum \\limits_{w \\in \\text {Vocab}} {y_w \\log \\hat {y}_w} = - \\log \\hat {y}_o$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "$y_w$ is non zero and equal to 1 only in the position $k$ of true word $o$. So left part can be simplified and be written as the only non zero term of a sum: $- \\sum \\limits_{w \\in \\text {Vocab}} {y_w \\log \\hat {y}_w} = - \\sum \\limits_{w = o} {y_w \\log \\hat {y}_w} = - y_o \\log \\hat{y}_o = - 1 * \\log \\hat{y}_o = - \\log \\hat{y}_o$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b) Compute the partial derivative of $J_{\\text {naive-softmax}}(v_c, o, U)$ with respect to $v_c$ . Please write your answer in terms of $y$, $\\hat{y}$, and $U$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "$\\frac {\\partial J} {\\partial v_c} = (\\hat{y} - y)U^T$\n",
    "\n",
    "Numerical check (loss should be decreasing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: 2.0011823067785066 v_c [0.65965599 0.17352562 0.16775718 0.59073473]\n",
      "gradient [ 0.26157565 -0.40633321  0.42686383  0.41312018]\n",
      "gradient check: [-1.71048108e-07 -2.99075358e-07 -4.82647708e-07 -3.36033180e-07]\n",
      "iteration 1\n",
      "Loss: 1.459945148371363 v_c [ 0.39808033  0.57985882 -0.25910666  0.17761455]\n",
      "gradient [ 0.21386538 -0.34613217  0.35182788  0.35564653]\n",
      "gradient check: [-1.72205029e-07 -3.09848814e-07 -5.00205743e-07 -4.02530177e-07]\n",
      "iteration 2\n",
      "Loss: 1.0801515250943454 v_c [ 0.18421495  0.925991   -0.61093454 -0.17803198]\n",
      "gradient [ 0.17248984 -0.28981979  0.2848892   0.2970376 ]\n",
      "gradient check: [-1.61428890e-07 -3.10479200e-07 -4.76503942e-07 -4.29432120e-07]\n",
      "iteration 3\n",
      "Loss: 0.8222445581074181 v_c [ 0.01172511  1.21581079 -0.89582374 -0.47506958]\n",
      "gradient [ 0.13958321 -0.24185402  0.23081792  0.24541722]\n",
      "gradient check: [-1.44873841e-07 -2.99073512e-07 -4.30959371e-07 -4.21111431e-07]\n",
      "iteration 4\n",
      "Loss: 0.6472140435603255 v_c [-0.12785809  1.45766481 -1.12664166 -0.7204868 ]\n",
      "gradient [ 0.11460528 -0.20343499  0.18944305  0.20373367]\n",
      "gradient check: [-1.27519925e-07 -2.80054224e-07 -3.80430055e-07 -3.93302966e-07]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def J(y, U, v_c):\n",
    "    #softmax value for each word in vocabulary. Array shape [1,V], where V - size of vocabulary\n",
    "    y_hat = np.exp(U.T @ v_c) / np.sum(np.exp(U.T @ v_c))\n",
    "    #loss function. Scalar value.\n",
    "    J = np.sum(- y * np.log(y_hat))\n",
    "    return J\n",
    "\n",
    "def grad_v_c(y, U, v_c):\n",
    "    y_hat = np.exp(U.T @ v_c) / np.sum(np.exp(U.T @ v_c))\n",
    "    #sum over rows. so we get shape [1,d], where d is the size of our vector representation\n",
    "    d = np.sum((y_hat-y)[:,np.newaxis]*U.T, axis=0)\n",
    "    return d\n",
    "\n",
    "#some random matrix U (suppose d=4, v=5, so shape is 4x5)\n",
    "U = np.array(np.random.random((4,5)))\n",
    "\n",
    "#some random word vector V_c\n",
    "v_c = np.random.random(4)\n",
    "\n",
    "#some random vector y with correct labels of words from vocabulary (size v, one-hot encoded)\n",
    "y = np.array([0,0,1,0,0])\n",
    "\n",
    "#matrix with small shift for every element in word_vector v_c to perform finite differences check\n",
    "h = np.diag([0.00001]*4)\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(y,U,v_c)\n",
    "    dj = grad_v_c(y,U,v_c)\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j,\"v_c\",v_c)\n",
    "    print(\"gradient\",dj)\n",
    "    print(\"gradient check:\", dj - [(J(y,U,v_c+h[0])-j)/np.sum(h[0]),\n",
    "                                   (J(y,U,v_c+h[1])-j)/np.sum(h[1]),\n",
    "                                   (J(y,U,v_c+h[2])-j)/np.sum(h[2]),\n",
    "                                   (J(y,U,v_c+h[3])-j)/np.sum(h[3])])\n",
    "    v_c = v_c - dj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c) Compute the partial derivatives of $J_{\\text naive-softmax} (v_c, o, U)$ with respect to each of the ‘outside’\n",
    "word vectors, $u_w$ ’s. There will be two cases: when $w = o$, the true ‘outside’ word vector, and $w \\ne o$, for\n",
    "all other words. Please write you answer in terms of $y$, $\\hat{y}$, and $v_c$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$$\\frac {\\partial J} {\\partial U} = v_c (\\hat{y} - y)^T$$ \n",
    "\n",
    "Numerical check (loss should be decreasing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: 2.0432893278345134 v_c [0.63326141 0.3609223  0.6198098  0.5393403 ]\n",
      "gradient [[ 0.09212016  0.09275454 -0.55118965  0.12842835  0.23788659]\n",
      " [ 0.05250315  0.05286471 -0.31414616  0.07319672  0.13558157]\n",
      " [ 0.09016337  0.09078427 -0.53948139  0.1257003   0.23283345]\n",
      " [ 0.07845752  0.07899781 -0.46944088  0.10938072  0.20260484]]\n",
      "gradient check: [[-2.49288763e-07 -2.50669257e-07 -2.26188589e-07 -3.24172154e-07\n",
      "  -4.70287100e-07]\n",
      " [-8.09877885e-08 -8.14829695e-08 -7.35110741e-08 -1.05339711e-07\n",
      "  -1.52772112e-07]\n",
      " [-2.38810876e-07 -2.40145070e-07 -2.16695342e-07 -3.10554288e-07\n",
      "  -4.50540259e-07]\n",
      " [-1.80823763e-07 -1.81827119e-07 -1.64088170e-07 -2.35155596e-07\n",
      "  -3.41134131e-07]]\n",
      "iteration 1\n",
      "Loss: 1.0059356135417779 v_c [0.63326141 0.3609223  0.6198098  0.5393403 ]\n",
      "gradient [[ 0.07632257  0.07675535 -0.40167624  0.09929353  0.1493048 ]\n",
      " [ 0.04349944  0.0437461  -0.22893218  0.05659156  0.08509508]\n",
      " [ 0.07470134  0.07512492 -0.39314392  0.09718436  0.14613329]\n",
      " [ 0.06500292  0.06537151 -0.3421023   0.08456698  0.1271609 ]]\n",
      "gradient check: [[-2.12521335e-07 -2.13590814e-07 -4.65094405e-07 -2.65097887e-07\n",
      "  -3.61281616e-07]\n",
      " [-6.90341290e-08 -6.93867667e-08 -1.51062091e-07 -8.61230651e-08\n",
      "  -1.17377380e-07]\n",
      " [-2.03596578e-07 -2.04611284e-07 -4.45580458e-07 -2.53946277e-07\n",
      "  -3.46102454e-07]\n",
      " [-1.54165752e-07 -1.54911900e-07 -3.37358564e-07 -1.92285769e-07\n",
      "  -2.62086348e-07]]\n",
      "iteration 2\n",
      "Loss: 0.5047721745423854 v_c [0.63326141 0.3609223  0.6198098  0.5393403 ]\n",
      "gradient [[ 0.05068165  0.05092703 -0.25099754  0.06311238  0.08627647]\n",
      " [ 0.0288856   0.02902546 -0.14305405  0.0359704   0.04917259]\n",
      " [ 0.04960508  0.04984525 -0.2456659   0.06177176  0.08444381]\n",
      " [ 0.04316489  0.04337388 -0.21377126  0.05375197  0.07348052]]\n",
      "gradient check: [[-1.47628723e-07 -1.48275536e-07 -4.79726810e-07 -1.79902633e-07\n",
      "  -2.35957648e-07]\n",
      " [-4.79431983e-08 -4.81586493e-08 -1.55806431e-07 -5.84248401e-08\n",
      "  -7.66351166e-08]\n",
      " [-1.41428215e-07 -1.42034041e-07 -4.59562609e-07 -1.72343349e-07\n",
      "  -2.26029872e-07]\n",
      " [-1.07080436e-07 -1.07544678e-07 -3.47976695e-07 -1.30482696e-07\n",
      "  -1.71157505e-07]]\n",
      "iteration 3\n",
      "Loss: 0.30678875921193705 v_c [0.63326141 0.3609223  0.6198098  0.5393403 ]\n",
      "gradient [[ 0.03477368  0.03492571 -0.16730385  0.04228929  0.05531517]\n",
      " [ 0.01981898  0.01990563 -0.0953535   0.02410244  0.03152644]\n",
      " [ 0.03403503  0.03418383 -0.16375002  0.04139099  0.05414017]\n",
      " [ 0.02961628  0.02974577 -0.14249046  0.03601722  0.04711119]]\n",
      "gradient check: [[-1.04063833e-07 -1.04488237e-07 -3.89774793e-07 -1.24953684e-07\n",
      "  -1.59846359e-07]\n",
      " [-3.38114340e-08 -3.39503491e-08 -1.26618431e-07 -4.05999540e-08\n",
      "  -5.19236660e-08]\n",
      " [-9.96897858e-08 -1.00090885e-07 -3.73411098e-07 -1.19706788e-07\n",
      "  -1.53130245e-07]\n",
      " [-7.54902616e-08 -7.58014152e-08 -2.82719491e-07 -9.06446237e-08\n",
      "  -1.15953995e-07]]\n",
      "iteration 4\n",
      "Loss: 0.21538398788489202 v_c [0.63326141 0.3609223  0.6198098  0.5393403 ]\n",
      "gradient [[ 0.02592781  0.02603363 -0.12270592  0.03108335  0.03966114]\n",
      " [ 0.01477735  0.01483766 -0.06993527  0.01771571  0.02260455]\n",
      " [ 0.02537706  0.02548063 -0.12009943  0.03042308  0.03881866]\n",
      " [ 0.02208237  0.02217249 -0.104507    0.02647327  0.03377886]]\n",
      "gradient check: [[-7.87431874e-08 -7.90437853e-08 -3.13249962e-07 -9.35837680e-08\n",
      "  -1.17716817e-07]\n",
      " [-2.55878844e-08 -2.56776558e-08 -1.01749717e-07 -3.04028166e-08\n",
      "  -3.82351779e-08]\n",
      " [-7.54411892e-08 -7.57239329e-08 -3.00088308e-07 -8.96576247e-08\n",
      "  -1.12771608e-07]\n",
      " [-5.71096820e-08 -5.73278815e-08 -2.27222368e-07 -6.78831049e-08\n",
      "  -8.53916129e-08]]\n"
     ]
    }
   ],
   "source": [
    "def grad_U(y, U, v_c):\n",
    "    y_hat = np.exp(U.T @ v_c) / np.sum(np.exp(U.T @ v_c))\n",
    "    #sum over rows. so we get shape [1,d], where d is the size of our vector representation\n",
    "    d = v_c.T[:,np.newaxis] @ (y_hat-y).T[np.newaxis,:]\n",
    "    return d\n",
    "\n",
    "#some random matrix U (suppose d=4, v=5, so shape is 4x5)\n",
    "U = np.array(np.random.random((4,5)))\n",
    "\n",
    "#some random word vector V_c\n",
    "v_c = np.random.random(4)\n",
    "\n",
    "#some random vector y with correct labels of words from vocabulary (size v, one-hot encoded)\n",
    "y = np.array([0,0,1,0,0])\n",
    "\n",
    "#matrix with small shift for every element in word_vector v_c to perform finite differences check\n",
    "h = np.zeros((20,4,5))\n",
    "for i in range(20):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    h[i,row,col] = 0.00001\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(y,U,v_c)\n",
    "    dj = grad_U(y,U,v_c)\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j,\"v_c\",v_c)\n",
    "    print(\"gradient\",dj)\n",
    "    diff = np.zeros_like(U)\n",
    "    for i in range(20):\n",
    "        row = i // 5\n",
    "        col = i % 5\n",
    "        diff[row,col] = (J(y,U+h[i],v_c)-j)/np.sum(h[i])\n",
    "    print(\"gradient check:\", dj - diff)\n",
    "    U = U - dj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d) The sigmoid function is given by equation:\n",
    "$$ \\sigma(x) = \\frac 1 {1 + e^{-x}} = \\frac {e^x} {e^x + 1} $$\n",
    "Please compute the derivative of $\\sigma(x)$ with respect to $x$, where $x$ is a vector.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$$\\sigma^\\prime(x) = \\frac {(e^x)^\\prime (e^x + 1) - e^x (e^x + 1)^\\prime} {(e^x + 1)^2} = \\frac {e^x (e^x + 1) - e^x e^x} {(e^x + 1)^2} = \\frac {e^x} {(e^x + 1)^2} = \\frac {e^x} {e^x + 1} \\frac 1 {e^x + 1} = \\sigma(x) (1 - \\frac {e^x} {e^x + 1}) = \\sigma(x) (1 - \\sigma(x))$$\n",
    "\n",
    "Numerical check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [0.31206212 0.45579732 0.22146572 0.56511716]\n",
      "Sigmoid value: [0.57738852 0.61201671 0.55514124 0.63763572]\n",
      "Gradient for x [0.24401102 0.23745226 0.24695944 0.23105641]\n",
      "gradient check: [1.88855363e-06 2.66003045e-06 1.36196329e-06 3.18031137e-06]\n",
      "X [0.48288811 0.03497572 0.861825   0.57506651]\n",
      "Sigmoid value: [0.61842963 0.50874304 0.70304181 0.63993142]\n",
      "Gradient for x [0.23597442 0.24992356 0.20877402 0.2304192 ]\n",
      "gradient check: [2.79480048e-06 2.18717212e-07 4.23907474e-06 3.22443490e-06]\n",
      "X [0.48865765 0.43121337 0.31837969 0.61015575]\n",
      "Sigmoid value: [0.61979016 0.60616337 0.57892932 0.64797633]\n",
      "Gradient for x [0.23565032 0.23872934 0.24377016 0.22810301]\n",
      "gradient check: [2.82302096e-06 2.53460294e-06 1.92425042e-06 3.37552403e-06]\n",
      "X [0.92942287 0.42928427 0.15898609 0.93813337]\n",
      "Sigmoid value: [0.71695818 0.60570274 0.53966301 0.71872245]\n",
      "Gradient for x [0.20292915 0.23882693 0.24842685 0.20216049]\n",
      "gradient check: [4.40278851e-06 2.52463943e-06 9.85538183e-07 4.42177640e-06]\n",
      "X [0.6704506  0.29246961 0.32517518 0.50558753]\n",
      "Sigmoid value: [0.66160405 0.57260063 0.58058496 0.62377152]\n",
      "Gradient for x [0.22388413 0.24472915 0.24350606 0.23468061]\n",
      "gradient check: [3.61818645e-06 1.77693999e-06 1.96247967e-06 2.90483766e-06]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    #naive sigmoid implementation\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def grad_sigmoid(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "#matrix with small shift for every element in vector X to perform finite differences check\n",
    "h = np.array([0.0001]*4)\n",
    "\n",
    "for i in range(5):\n",
    "    #some random vector X (suppose d=4)\n",
    "    X = np.random.random(4)\n",
    "    s = sigmoid(X)\n",
    "    ds = grad_sigmoid(X)\n",
    "    print(\"X\",X)\n",
    "    print(\"Sigmoid value:\",s)\n",
    "    print(\"Gradient for x\",ds)\n",
    "    print(\"gradient check:\", ds - (sigmoid(X+h)-s)/h[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e) Now we shall consider the Negative Sampling loss, which is an alternative to the Naive Softmax loss. Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1$, $w_2$, ..., $w_K$ and their outside vectors as $u_1$, ..., $u_K$. Note that $o \\notin \\{w_1, ..., w_K\\}$. For a center word $c$ and an outside word $o$, the negative sampling loss function is given by:__\n",
    "\n",
    "$$J_{\\text neg-sample}(v_c, o, U) = − \\log(\\sigma(u_o v_c)) − \\sum\\limits_K {\\log(\\sigma(−u_k v_c))}$$\n",
    "\n",
    "__for a sample $w_1$, ..., $w_K$, where $\\sigma(x)$ is the sigmoid function. Please repeat parts (b) and (c), computing the partial derivatives of $J_{\\text neg-sample}$ with respect to $v_c$, with respect to $u_o$, and with respect to a negative sample $u_k$. Please write your answers in terms of the vectors $u_o$, $v_c$ and $u_k$, where $k \\in [1,K]$.__\n",
    "\n",
    "__After you’ve done this, describe with one sentence why this loss function is much more efficient to compute than the naive-softmax loss. Note, you should be able to use your solution to part (d) to help compute the necessary gradients here.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$$\\frac {\\partial{J}} {\\partial{u_o}} = - (1 - \\sigma(u_o^T v_c))v_c$$\n",
    "\n",
    "$$\\frac {\\partial{J}} {\\partial{u_k}} = (1 - \\sigma(-u_k^T v_c))v_c \\\\ \n",
    "\\text{or in matrix form} \\\\ \n",
    "\\frac {\\partial{J}} {\\partial{U_k}} = v_c \\times (1 - \\sigma(-U_k^T v_c))^T$$\n",
    "\n",
    "$$\\frac {\\partial{J}} {\\partial{v_c}} = - (1 - \\sigma(u_o^T v_c)) u_o + \\sum\\limits_K {(1 - \\sigma(-u_k^T v_c))u_k} \\\\ \n",
    "\\text{or in matrix form} \\\\ \n",
    "\\frac {\\partial{J}} {\\partial{v_c}} = - (1 - \\sigma(u_o^T v_c)) u_o + U_k \\times (1 - \\sigma(-U_k^T v_c))$$\n",
    "\n",
    "Numerical check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(u_o, u, v_c):\n",
    "    J = -np.log(sigmoid(u_o.T @ v_c)) - np.sum(np.log(sigmoid(-u.T @ v_c)), axis = 0)\n",
    "    return J.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: [6.07813947]\n",
      "gradient [[-0.19303651]\n",
      " [-0.19234558]\n",
      " [-0.08921333]\n",
      " [-0.21368096]]\n",
      "gradient check: [[-5.57318569e-06]\n",
      " [-5.53336025e-06]\n",
      " [-1.19038373e-06]\n",
      " [-6.82897454e-06]]\n",
      "iteration 1\n",
      "Loss: [5.97260761]\n",
      "gradient [[-0.12876768]\n",
      " [-0.12830678]\n",
      " [-0.05951099]\n",
      " [-0.14253884]]\n",
      "gradient check: [[-4.13143483e-06]\n",
      " [-4.10190960e-06]\n",
      " [-8.82437264e-07]\n",
      " [-5.06235576e-06]]\n",
      "iteration 2\n",
      "Loss: [5.92317826]\n",
      "gradient [[-0.09625192]\n",
      " [-0.09590741]\n",
      " [-0.04448357]\n",
      " [-0.10654566]]\n",
      "gradient check: [[-3.24465983e-06]\n",
      " [-3.22147360e-06]\n",
      " [-6.93034122e-07]\n",
      " [-3.97576532e-06]]\n",
      "iteration 3\n",
      "Loss: [5.89470655]\n",
      "gradient [[-0.07677999]\n",
      " [-0.07650518]\n",
      " [-0.03548447]\n",
      " [-0.08499129]]\n",
      "gradient check: [[-2.66300498e-06]\n",
      " [-2.64397413e-06]\n",
      " [-5.68790962e-07]\n",
      " [-3.26304674e-06]]\n",
      "iteration 4\n",
      "Loss: [5.87622734]\n",
      "gradient [[-0.06384204]\n",
      " [-0.06361353]\n",
      " [-0.0295051 ]\n",
      " [-0.07066968]]\n",
      "gradient check: [[-2.25557089e-06]\n",
      " [-2.23945667e-06]\n",
      " [-4.81775591e-07]\n",
      " [-2.76380689e-06]]\n"
     ]
    }
   ],
   "source": [
    "def grad_U_o(u_o, v_c):\n",
    "    return -(1 - sigmoid(u_o.T @ v_c))*v_c\n",
    "\n",
    "u_o = np.random.random((4,1))\n",
    "u = np.random.random((4,3))\n",
    "v_c = np.random.random((4,1))\n",
    "h = np.diag([0.0001]*4)\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(u_o,u,v_c)\n",
    "    dj = grad_U_o(u_o,v_c).reshape((4,1))\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j)\n",
    "    print(\"gradient\",dj)\n",
    "    print(\"gradient check:\", dj - np.array([(J(u_o+h[0].reshape((-1,1)),u,v_c)-j)/np.sum(h[0]),\n",
    "                                            (J(u_o+h[1].reshape((-1,1)),u,v_c)-j)/np.sum(h[1]),\n",
    "                                            (J(u_o+h[2].reshape((-1,1)),u,v_c)-j)/np.sum(h[2]),\n",
    "                                            (J(u_o+h[3].reshape((-1,1)),u,v_c)-j)/np.sum(h[3])]))\n",
    "    u_o = u_o - dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: [4.00174345]\n",
      "gradient [[0.08410885 0.08734542 0.08612594]\n",
      " [0.27522971 0.28582075 0.28183023]\n",
      " [0.57606874 0.59823629 0.58988395]\n",
      " [0.11556171 0.12000861 0.11833309]]\n",
      "gradient check: [[-1.58441093e-07 -1.50402988e-07 -1.53557925e-07]\n",
      " [-1.69661183e-06 -1.61053867e-06 -1.64428377e-06]\n",
      " [-7.43253846e-06 -7.05547401e-06 -7.20331500e-06]\n",
      " [-2.99100117e-07 -2.83929832e-07 -2.89877106e-07]]\n",
      "iteration 1\n",
      "Loss: [2.79749007]\n",
      "gradient [[0.06645817 0.06957243 0.06837995]\n",
      " [0.21747131 0.22766214 0.22375996]\n",
      " [0.4551777  0.47650758 0.46834014]\n",
      " [0.09131048 0.09558934 0.09395092]]\n",
      "gradient check: [[-1.83845701e-07 -1.81628859e-07 -1.82587124e-07]\n",
      " [-1.96860731e-06 -1.94485756e-06 -1.95517794e-06]\n",
      " [-8.62416497e-06 -8.52009592e-06 -8.56532466e-06]\n",
      " [-3.47055095e-07 -3.42863840e-07 -3.44685901e-07]]\n",
      "iteration 2\n",
      "Loss: [2.04854715]\n",
      "gradient [[0.05163626 0.05405248 0.05311973]\n",
      " [0.16896954 0.17687613 0.17382391]\n",
      " [0.35366119 0.37021007 0.36382163]\n",
      " [0.07094586 0.07426563 0.07298408]]\n",
      "gradient check: [[-1.81112048e-07 -1.83057719e-07 -1.82377993e-07]\n",
      " [-1.93933306e-06 -1.96015153e-06 -1.95285981e-06]\n",
      " [-8.49592904e-06 -8.58714108e-06 -8.55517727e-06]\n",
      " [-3.41890706e-07 -3.45566693e-07 -3.44278775e-07]]\n",
      "iteration 3\n",
      "Loss: [1.59454333]\n",
      "gradient [[0.04076467 0.04249001 0.04182268]\n",
      " [0.13339438 0.13904021 0.1368565 ]\n",
      " [0.27920072 0.29101771 0.2864471 ]\n",
      " [0.05600879 0.05837932 0.05746244]]\n",
      "gradient check: [[-1.65138409e-07 -1.68462170e-07 -1.67213726e-07]\n",
      " [-1.76830147e-06 -1.80389361e-06 -1.79050671e-06]\n",
      " [-7.74669541e-06 -7.90261398e-06 -7.84396441e-06]\n",
      " [-3.11741277e-07 -3.18016820e-07 -3.15658748e-07]]\n",
      "iteration 4\n",
      "Loss: [1.30970788]\n",
      "gradient [[0.03305884 0.03428327 0.03381004]\n",
      " [0.10817855 0.11218528 0.11063671]\n",
      " [0.22642281 0.23480906 0.23156784]\n",
      " [0.04542133 0.04710364 0.04645344]]\n",
      "gradient check: [[-1.46657925e-07 -1.49991926e-07 -1.48721562e-07]\n",
      " [-1.57042874e-06 -1.60612055e-06 -1.59251303e-06]\n",
      " [-6.87985428e-06 -7.03620954e-06 -6.97661241e-06]\n",
      " [-2.76854823e-07 -2.83147988e-07 -2.80749699e-07]]\n"
     ]
    }
   ],
   "source": [
    "def grad_U(u, v_c):\n",
    "    return v_c @ (1 - sigmoid(-u.T @ v_c)).T\n",
    "\n",
    "u_o = np.random.random((4,1))\n",
    "u = np.random.random((4,3))\n",
    "v_c = np.random.random((4,1))\n",
    "h = np.zeros((12,4,3))\n",
    "for i in range(12):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    h[i,row,col] = 0.0001\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(u_o,u,v_c)\n",
    "    dj = grad_U(u,v_c)\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j)\n",
    "    print(\"gradient\",dj)\n",
    "    diff = np.zeros_like(u)\n",
    "    for i in range(12):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        diff[row,col] = (J(u_o,u+h[i],v_c)-j)/np.sum(h[i])\n",
    "    print(\"gradient check:\", dj - diff)\n",
    "    u = u - dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: [3.42952005]\n",
      "gradient [[0.15843803]\n",
      " [0.67268655]\n",
      " [0.7644406 ]\n",
      " [0.49613987]]\n",
      "gradient check: [[-3.93701870e-06]\n",
      " [-7.80428311e-06]\n",
      " [-1.60732726e-05]\n",
      " [-8.63920169e-06]]\n",
      "iteration 1\n",
      "Loss: [2.60456207]\n",
      "gradient [[-0.12619046]\n",
      " [ 0.27975382]\n",
      " [ 0.14575329]\n",
      " [ 0.03940952]]\n",
      "gradient check: [[-5.45835467e-06]\n",
      " [-9.42912001e-06]\n",
      " [-2.08298835e-05]\n",
      " [-1.14212178e-05]]\n",
      "iteration 2\n",
      "Loss: [2.50534519]\n",
      "gradient [[-0.17796141]\n",
      " [ 0.20167916]\n",
      " [ 0.03102633]\n",
      " [-0.04452132]]\n",
      "gradient check: [[-5.31022726e-06]\n",
      " [-9.18296141e-06]\n",
      " [-2.00781510e-05]\n",
      " [-1.10417578e-05]]\n",
      "iteration 3\n",
      "Loss: [2.43123423]\n",
      "gradient [[-0.18229266]\n",
      " [ 0.18634234]\n",
      " [ 0.01667763]\n",
      " [-0.05384099]]\n",
      "gradient check: [[-5.30961325e-06]\n",
      " [-9.11101527e-06]\n",
      " [-1.99573277e-05]\n",
      " [-1.09987201e-05]]\n",
      "iteration 4\n",
      "Loss: [2.36094051]\n",
      "gradient [[-0.18044678]\n",
      " [ 0.17938176]\n",
      " [ 0.01528714]\n",
      " [-0.05353138]]\n",
      "gradient check: [[-5.33081499e-06]\n",
      " [-9.06585886e-06]\n",
      " [-1.99372555e-05]\n",
      " [-1.10083244e-05]]\n"
     ]
    }
   ],
   "source": [
    "def grad_v_c(u_o, u, v_c):\n",
    "    return -(1 - sigmoid(u_o.T @ v_c))*u_o + u @ (1 - sigmoid(-u.T @ v_c))\n",
    "\n",
    "u_o = np.random.random((4,1))\n",
    "u = np.random.random((4,3))\n",
    "v_c = np.random.random((4,1))\n",
    "h = np.diag([0.0001]*4)\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(u_o,u,v_c)\n",
    "    dj = grad_v_c(u_o, u, v_c)\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j)\n",
    "    print(\"gradient\",dj)\n",
    "    print(\"gradient check:\", dj - np.array([(J(u_o,u,v_c+h[0].reshape((-1,1)))-j)/np.sum(h[0]),\n",
    "                                            (J(u_o,u,v_c+h[1].reshape((-1,1)))-j)/np.sum(h[1]),\n",
    "                                            (J(u_o,u,v_c+h[2].reshape((-1,1)))-j)/np.sum(h[2]),\n",
    "                                            (J(u_o,u,v_c+h[3].reshape((-1,1)))-j)/np.sum(h[3])]))\n",
    "    v_c = v_c - dj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__f) Suppose the center word is $c = w_t$ and the context window is $[w_{t−m}, ..., w_{t−1}, w_t, w_{t+1}, ...,\n",
    "w_{t+m}]$, where $m$ is the context window size. Recall that for the skip-gram version of word2vec, the total loss for the context window is:__\n",
    "\n",
    "$$J_{\\text skip-gram}(v_c, w_{t−m}, ..., w_{t+m}, U) = \\sum\\limits_{-m \\le j \\le m} {J(v_c, w_{t+j}, U)}$$\n",
    "\n",
    "__Here, $J(v_c, w_{t+j}, U)$ represents an arbitrary loss term for the center word $c = w_t$ and outside word $w_{t+j}$. $J(v_c, w_{t+j}, U)$ could be $J_{\\text naive-softmax}(v_c, w_{t+j}, U)$ or $J_{\\text neg-sample}(v_c, w_{t+j}, U)$, depending on your implementation.__\n",
    "\n",
    "__Write down three partial derivatives:__\n",
    "- $\\frac {\\partial J_{\\text skip-gram}} {\\partial U}$\n",
    "- $\\frac {\\partial J_{\\text skip-gram}} {\\partial v_c}$\n",
    "- $\\frac {\\partial J_{\\text skip-gram}} {\\partial v_w} \\text {, where } w \\ne c$\n",
    "\n",
    "__Write your answers in terms of $\\frac {\\partial {J(v_c, w_{t+j}, U)}} {\\partial U}$ and $\\frac {\\partial {J(v_c, w_{t+j}, U)}} {\\partial v_c}$__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$$\\frac {\\partial J_{\\text skip-gram}} {\\partial U} = \\sum\\limits_{-m \\le j \\le m} {\\frac {\\partial {J(v_c, w_{t+j}, U)}} {\\partial U}}$$\n",
    "\n",
    "$$\\frac {\\partial J_{\\text skip-gram}} {\\partial v_c} = \\sum\\limits_{-m \\le j \\le m} {\\frac {\\partial {J(v_c, w_{t+j}, U)}} {\\partial v_c}}$$\n",
    "\n",
    "$$\\frac {\\partial J_{\\text skip-gram}} {\\partial v_w} = 0 \\text {, for all } w \\ne c $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
