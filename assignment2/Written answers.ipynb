{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a) Show that the naive-softmax loss given in $J_{\\text {naive_softmax}}(v_c, o, U) = - \\log P(O=o|C=c)$ is the same as the cross-entropy loss between $y$ and $\\hat{y}$; i.e., show that:__\n",
    "\n",
    "$$- \\sum_{w \\in \\text {Vocab}} {y_w \\log \\hat {y}_w} = - \\log \\hat {y}_o$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "$y_w$ is non zero and equal to 1 only in the position $k$ of true word $o$. So left part can be simplified and be written as the only non zero term of a sum: \n",
    "\n",
    "$$- \\sum_{w \\in \\text {Vocab}} {y_w \\log \\hat {y}_w} = - \\sum_{w = o} {y_w \\log \\hat {y}_w} = - y_o \\log \\hat{y}_o = - 1 * \\log \\hat{y}_o = - \\log \\hat{y}_o$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b) Compute the partial derivative of $J_{\\text {naive-softmax}}(v_c, o, U)$ with respect to $v_c$ . Please write your answer in terms of $y$, $\\hat{y}$, and $U$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "$\\frac {\\partial J} {\\partial v_c} = (\\hat{y} - y)U^T$\n",
    "\n",
    "Numerical check (loss should be decreasing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: 1.396645847775709 v_c [0.3277639  0.2355097  0.63730662 0.45045224]\n",
      "gradient [-0.17616601  0.2021204  -0.21494512 -0.10431832]\n",
      "gradient check: [-2.57400052e-07 -4.90028697e-07 -2.87680664e-07 -2.42110742e-07]\n",
      "iteration 1\n",
      "Loss: 1.2765575948229395 v_c [0.50392991 0.0333893  0.85225174 0.55477056]\n",
      "gradient [-0.15197113  0.16622224 -0.18520328 -0.1070215 ]\n",
      "gradient check: [-2.29392500e-07 -4.93436727e-07 -2.65967333e-07 -2.27544151e-07]\n",
      "iteration 2\n",
      "Loss: 1.1860828542365922 v_c [ 0.65590104 -0.13283294  1.03745501  0.66179206]\n",
      "gradient [-0.13297829  0.13725236 -0.16194976 -0.10813718]\n",
      "gradient check: [-2.04244209e-07 -4.87419743e-07 -2.45045411e-07 -2.13958033e-07]\n",
      "iteration 3\n",
      "Loss: 1.115752711021939 v_c [ 0.78887933 -0.2700853   1.19940477  0.76992924]\n",
      "gradient [-0.11793785  0.1140384  -0.14368071 -0.10823274]\n",
      "gradient check: [-1.82480118e-07 -4.76700683e-07 -2.26278084e-07 -2.01832791e-07]\n",
      "iteration 4\n",
      "Loss: 1.0593584938930638 v_c [ 0.90681718 -0.3841237   1.34308548  0.87816198]\n",
      "gradient [-0.10586797  0.09542034 -0.12917925 -0.10767362]\n",
      "gradient check: [-1.63743697e-07 -4.64046389e-07 -2.09776631e-07 -1.91176662e-07]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def J(y, U, v_c):\n",
    "    #softmax value for each word in vocabulary. Array shape [1,V], where V - size of vocabulary\n",
    "    y_hat = np.exp(U.T @ v_c) / np.sum(np.exp(U.T @ v_c))\n",
    "    #loss function. Scalar value.\n",
    "    J = np.sum(- y * np.log(y_hat))\n",
    "    return J\n",
    "\n",
    "def grad_v_c(y, U, v_c):\n",
    "    y_hat = np.exp(U.T @ v_c) / np.sum(np.exp(U.T @ v_c))\n",
    "    #sum over rows. so we get shape [1,d], where d is the size of our vector representation\n",
    "    d = np.sum((y_hat-y)[:,np.newaxis]*U.T, axis=0)\n",
    "    return d\n",
    "\n",
    "#some random matrix U (suppose d=4, v=5, so shape is 4x5)\n",
    "U = np.array(np.random.random((4,5)))\n",
    "\n",
    "#some random word vector V_c\n",
    "v_c = np.random.random(4)\n",
    "\n",
    "#some random vector y with correct labels of words from vocabulary (size v, one-hot encoded)\n",
    "y = np.array([0,0,1,0,0])\n",
    "\n",
    "#matrix with small shift for every element in word_vector v_c to perform finite differences check\n",
    "h = np.diag([0.00001]*4)\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(y,U,v_c)\n",
    "    dj = grad_v_c(y,U,v_c)\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j,\"v_c\",v_c)\n",
    "    print(\"gradient\",dj)\n",
    "    print(\"gradient check:\", dj - [(J(y,U,v_c+h[0])-j)/np.sum(h[0]),\n",
    "                                   (J(y,U,v_c+h[1])-j)/np.sum(h[1]),\n",
    "                                   (J(y,U,v_c+h[2])-j)/np.sum(h[2]),\n",
    "                                   (J(y,U,v_c+h[3])-j)/np.sum(h[3])])\n",
    "    v_c = v_c - dj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c) Compute the partial derivatives of $J_{\\text naive-softmax} (v_c, o, U)$ with respect to each of the ‘outside’\n",
    "word vectors, $u_w$ ’s. There will be two cases: when $w = o$, the true ‘outside’ word vector, and $w \\ne o$, for\n",
    "all other words. Please write you answer in terms of $y$, $\\hat{y}$, and $v_c$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$$\\frac {\\partial J} {\\partial U} = v_c (\\hat{y} - y)^T$$ \n",
    "\n",
    "Numerical check (loss should be decreasing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: 2.246555386582317 v_c [0.70716958 0.36076553 0.60210062 0.27327096]\n",
      "gradient [[ 0.14966649  0.14402201 -0.63237727  0.15196379  0.18672498]\n",
      " [ 0.07635299  0.07347343 -0.32260992  0.07752497  0.09525853]\n",
      " [ 0.12742953  0.12262369 -0.5384207   0.1293855   0.15898199]\n",
      " [ 0.0578355   0.05565431 -0.24436903  0.05872324  0.07215598]]\n",
      "gradient check: [[-4.17208270e-07 -4.05524080e-07 -2.36476755e-07 -4.21894367e-07\n",
      "  -4.85900865e-07]\n",
      " [-1.08593897e-07 -1.05563286e-07 -6.15675031e-08 -1.09810992e-07\n",
      "  -1.26454771e-07]\n",
      " [-3.02471737e-07 -2.93976248e-07 -1.71463211e-07 -3.05837219e-07\n",
      "  -3.52266448e-07]\n",
      " [-6.23206819e-08 -6.05856751e-08 -3.53434825e-08 -6.30155270e-08\n",
      "  -7.25884637e-08]]\n",
      "iteration 1\n",
      "Loss: 1.2692155010583435 v_c [0.70716958 0.36076553 0.60210062 0.27327096]\n",
      "gradient [[ 0.12215456  0.11855346 -0.50841814  0.12360022  0.14410991]\n",
      " [ 0.06231766  0.06048054 -0.25937165  0.06305517  0.07351828]\n",
      " [ 0.10400523  0.10093917 -0.43287902  0.1052361   0.12269853]\n",
      " [ 0.04720409  0.04581251 -0.19646761  0.04776273  0.05568828]]\n",
      "gradient check: [[-3.57310618e-07 -3.48906464e-07 -5.05259747e-07 -3.60643302e-07\n",
      "  -4.05719983e-07]\n",
      " [-9.29902851e-08 -9.08031177e-08 -1.31518048e-07 -9.38546934e-08\n",
      "  -1.05599863e-07]\n",
      " [-2.59018728e-07 -2.52934722e-07 -3.66292701e-07 -2.61439917e-07\n",
      "  -2.94100354e-07]\n",
      " [-5.33603876e-08 -5.20954678e-08 -7.54417152e-08 -5.38585648e-08\n",
      "  -6.05631891e-08]]\n",
      "iteration 2\n",
      "Loss: 0.6826485047223624 v_c [0.70716958 0.36076553 0.60210062 0.27327096]\n",
      "gradient [[ 0.08477758  0.08272681 -0.34985307  0.08559391  0.09675477]\n",
      " [ 0.04324964  0.04220343 -0.17847901  0.04366609  0.04935985]\n",
      " [ 0.0721816   0.07043552 -0.29787303  0.07287664  0.08237926]\n",
      " [ 0.03276053  0.03196805 -0.13519343  0.03307598  0.03738887]]\n",
      "gradient check: [[-2.63816929e-07 -2.58290750e-07 -6.25042367e-07 -2.66007294e-07\n",
      "  -2.95309362e-07]\n",
      " [-6.86500624e-08 -6.72126684e-08 -1.62693061e-07 -6.92347963e-08\n",
      "  -7.68648351e-08]\n",
      " [-1.91255439e-07 -1.87230618e-07 -4.53108899e-07 -1.92828654e-07\n",
      "  -2.14052378e-07]\n",
      " [-3.93764653e-08 -3.85489117e-08 -9.33294213e-08 -3.97253886e-08\n",
      "  -4.40935530e-08]]\n",
      "iteration 3\n",
      "Loss: 0.40929579826984125 v_c [0.70716958 0.36076553 0.60210062 0.27327096]\n",
      "gradient [[ 0.05781956  0.05659583 -0.23752571  0.05830442  0.0648059 ]\n",
      " [ 0.02949689  0.0288726  -0.12117474  0.02974425  0.033061  ]\n",
      " [ 0.04922892  0.048187   -0.2022349   0.04964174  0.05517725]\n",
      " [ 0.02234317  0.02187028 -0.09178686  0.02253053  0.02504289]]\n",
      "gradient check: [[-1.87728703e-07 -1.84114853e-07 -5.57782542e-07 -1.89166445e-07\n",
      "  -2.08162669e-07]\n",
      " [-4.88672543e-08 -4.79219889e-08 -1.45169510e-07 -4.92299143e-08\n",
      "  -5.41906131e-08]\n",
      " [-1.36084166e-07 -1.33472456e-07 -4.04365368e-07 -1.37135010e-07\n",
      "  -1.50895740e-07]\n",
      " [-2.80432009e-08 -2.75076214e-08 -8.33019737e-08 -2.82542746e-08\n",
      "  -3.10954948e-08]]\n",
      "iteration 4\n",
      "Loss: 0.27989902649687415 v_c [0.70716958 0.36076553 0.60210062 0.27327096]\n",
      "gradient [[ 0.04213643  0.04132088 -0.17264834  0.04245869  0.04673234]\n",
      " [ 0.02149608  0.02108002 -0.08807728  0.02166048  0.0238407 ]\n",
      " [ 0.03587593  0.03518156 -0.14699681  0.03615031  0.039789  ]\n",
      " [ 0.01628275  0.01596759 -0.06671636  0.01640728  0.01805874]]\n",
      "gradient check: [[-1.40116096e-07 -1.37566946e-07 -4.61414268e-07 -1.41124794e-07\n",
      "  -1.54333669e-07]\n",
      " [-3.64641980e-08 -3.58036427e-08 -1.20093352e-07 -3.67313091e-08\n",
      "  -4.01587862e-08]\n",
      " [-1.01571704e-07 -9.97200542e-08 -3.34487036e-07 -1.02304080e-07\n",
      "  -1.11861677e-07]\n",
      " [-2.09087427e-08 -2.05414914e-08 -6.88991956e-08 -2.10785398e-08\n",
      "  -2.30484622e-08]]\n"
     ]
    }
   ],
   "source": [
    "def grad_U(y, U, v_c):\n",
    "    y_hat = np.exp(U.T @ v_c) / np.sum(np.exp(U.T @ v_c))\n",
    "    #sum over rows. so we get shape [1,d], where d is the size of our vector representation\n",
    "    d = v_c.T[:,np.newaxis] @ (y_hat-y).T[np.newaxis,:]\n",
    "    return d\n",
    "\n",
    "#some random matrix U (suppose d=4, v=5, so shape is 4x5)\n",
    "U = np.array(np.random.random((4,5)))\n",
    "\n",
    "#some random word vector V_c\n",
    "v_c = np.random.random(4)\n",
    "\n",
    "#some random vector y with correct labels of words from vocabulary (size v, one-hot encoded)\n",
    "y = np.array([0,0,1,0,0])\n",
    "\n",
    "#matrix with small shift for every element in word_vector v_c to perform finite differences check\n",
    "h = np.zeros((20,4,5))\n",
    "for i in range(20):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    h[i,row,col] = 0.00001\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(y,U,v_c)\n",
    "    dj = grad_U(y,U,v_c)\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j,\"v_c\",v_c)\n",
    "    print(\"gradient\",dj)\n",
    "    diff = np.zeros_like(U)\n",
    "    for i in range(20):\n",
    "        row = i // 5\n",
    "        col = i % 5\n",
    "        diff[row,col] = (J(y,U+h[i],v_c)-j)/np.sum(h[i])\n",
    "    print(\"gradient check:\", dj - diff)\n",
    "    U = U - dj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d) The sigmoid function is given by equation:\n",
    "$$ \\sigma(x) = \\frac 1 {1 + e^{-x}} = \\frac {e^x} {e^x + 1} $$\n",
    "Please compute the derivative of $\\sigma(x)$ with respect to $x$, where $x$ is a vector.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$$\\sigma^\\prime(x) = \\frac {(e^x)^\\prime (e^x + 1) - e^x (e^x + 1)^\\prime} {(e^x + 1)^2} = \\frac {e^x (e^x + 1) - e^x e^x} {(e^x + 1)^2} = \\frac {e^x} {(e^x + 1)^2} = \\frac {e^x} {e^x + 1} \\frac 1 {e^x + 1} = \\sigma(x) (1 - \\frac {e^x} {e^x + 1}) = \\sigma(x) (1 - \\sigma(x))$$\n",
    "\n",
    "Numerical check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [0.44669405 0.00276901 0.99923837 0.62713255]\n",
      "Sigmoid value: [0.60985293 0.50069225 0.73090881 0.65183899]\n",
      "Gradient for x [0.23793233 0.24999952 0.19668112 0.22694492]\n",
      "gradient check: [2.61392540e-06 1.75151171e-08 4.54159833e-06 3.44604698e-06]\n",
      "X [0.96305427 0.61912816 0.58678562 0.29704091]\n",
      "Sigmoid value: [0.7237329  0.65002024 0.64262728 0.57371898]\n",
      "Gradient for x [0.19994359 0.22749393 0.22965746 0.24456551]\n",
      "gradient check: [4.47346122e-06 3.41300807e-06 3.27568567e-06 1.80310283e-06]\n",
      "X [0.52834246 0.71798239 0.88569308 0.52539044]\n",
      "Sigmoid value: [0.62909643 0.67216257 0.70800058 0.62840737]\n",
      "Gradient for x [0.23333411 0.22036005 0.20673576 0.23351155]\n",
      "gradient check: [3.01241582e-06 3.79389377e-06 4.30019823e-06 2.99861661e-06]\n",
      "X [0.82416312 0.09784161 0.06883076 0.38658814]\n",
      "Sigmoid value: [0.69511934 0.52444091 0.5172009  0.59546109]\n",
      "Gradient for x [0.21192844 0.24940264 0.24970413 0.24088718]\n",
      "gradient check: [4.13522946e-06 6.09769257e-07 4.29720708e-07 2.29971461e-06]\n",
      "X [0.1102671  0.44161626 0.13397078 0.34540819]\n",
      "Sigmoid value: [0.52753888 0.60864409 0.53344269 0.58550363]\n",
      "Gradient for x [0.24924161 0.23819646 0.24888159 0.24268913]\n",
      "gradient check: [6.86589456e-07 2.58803415e-06 8.32531515e-07 2.07526433e-06]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    #naive sigmoid implementation\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def grad_sigmoid(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "#matrix with small shift for every element in vector X to perform finite differences check\n",
    "h = np.array([0.0001]*4)\n",
    "\n",
    "for i in range(5):\n",
    "    #some random vector X (suppose d=4)\n",
    "    X = np.random.random(4)\n",
    "    s = sigmoid(X)\n",
    "    ds = grad_sigmoid(X)\n",
    "    print(\"X\",X)\n",
    "    print(\"Sigmoid value:\",s)\n",
    "    print(\"Gradient for x\",ds)\n",
    "    print(\"gradient check:\", ds - (sigmoid(X+h)-s)/h[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e) Now we shall consider the Negative Sampling loss, which is an alternative to the Naive Softmax loss. Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1$, $w_2$, ..., $w_K$ and their outside vectors as $u_1$, ..., $u_K$. Note that $o \\notin \\{w_1, \\ldots, w_K\\}$. For a center word $c$ and an outside word $o$, the negative sampling loss function is given by:__\n",
    "\n",
    "$$J_{\\text neg-sample}(v_c, o, U) = − \\log(\\sigma(u_o v_c)) − \\sum_K {\\log(\\sigma(−u_k v_c))}$$\n",
    "\n",
    "__for a sample $w_1$, ..., $w_K$, where $\\sigma(x)$ is the sigmoid function. Please repeat parts (b) and (c), computing the partial derivatives of $J_{\\text neg-sample}$ with respect to $v_c$, with respect to $u_o$, and with respect to a negative sample $u_k$. Please write your answers in terms of the vectors $u_o$, $v_c$ and $u_k$, where $k \\in [1,K]$.__\n",
    "\n",
    "__After you’ve done this, describe with one sentence why this loss function is much more efficient to compute than the naive-softmax loss. Note, you should be able to use your solution to part (d) to help compute the necessary gradients here.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$$\\frac {\\partial{J}} {\\partial{u_o}} = - (1 - \\sigma(u_o^T v_c))v_c$$\n",
    "\n",
    "$$\\frac {\\partial{J}} {\\partial{u_k}} = (1 - \\sigma(-u_k^T v_c))v_c \\\\ \n",
    "\\text{or in matrix form} \\\\ \n",
    "\\frac {\\partial{J}} {\\partial{U_k}} = v_c \\times (1 - \\sigma(-U_k^T v_c))^T$$\n",
    "\n",
    "$$\\frac {\\partial{J}} {\\partial{v_c}} = - (1 - \\sigma(u_o^T v_c)) u_o + \\sum_K {(1 - \\sigma(-u_k^T v_c))u_k} \\\\ \n",
    "\\text{or in matrix form} \\\\ \n",
    "\\frac {\\partial{J}} {\\partial{v_c}} = - (1 - \\sigma(u_o^T v_c)) u_o + U_k \\times (1 - \\sigma(-U_k^T v_c))$$\n",
    "\n",
    "We need to calculate denominator in softmax-naive approach which is a summation over all corpus. It can be very inefficient. Instead in Negative Sampling we need to calculate sum over a small sample of negative words.\n",
    "\n",
    "Numerical check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(u_o, u, v_c):\n",
    "    J = -np.log(sigmoid(u_o.T @ v_c)) - np.sum(np.log(sigmoid(-u.T @ v_c)), axis = 0)\n",
    "    return J.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: [4.19528017]\n",
      "gradient [[-0.00896   ]\n",
      " [-0.19028008]\n",
      " [-0.10124527]\n",
      " [-0.19798658]]\n",
      "gradient check: [[-1.09191948e-08]\n",
      " [-4.92392033e-06]\n",
      " [-1.39404451e-06]\n",
      " [-5.33083867e-06]]\n",
      "iteration 1\n",
      "Loss: [4.11903751]\n",
      "gradient [[-0.00702925]\n",
      " [-0.14927748]\n",
      " [-0.07942838]\n",
      " [-0.15532334]]\n",
      "gradient check: [[-9.24857534e-09]\n",
      " [-4.16891238e-06]\n",
      " [-1.18029527e-06]\n",
      " [-4.51343474e-06]]\n",
      "iteration 2\n",
      "Loss: [4.0712289]\n",
      "gradient [[-0.00574127]\n",
      " [-0.12192523]\n",
      " [-0.06487465]\n",
      " [-0.12686331]]\n",
      "gradient check: [[-7.91848551e-09]\n",
      " [-3.57177811e-06]\n",
      " [-1.01122875e-06]\n",
      " [-3.86695212e-06]]\n",
      "iteration 3\n",
      "Loss: [4.03887326]\n",
      "gradient [[-0.004834  ]\n",
      " [-0.10265788]\n",
      " [-0.05462277]\n",
      " [-0.10681561]]\n",
      "gradient check: [[-6.88342674e-09]\n",
      " [-3.10622776e-06]\n",
      " [-8.79424876e-07]\n",
      " [-3.36293252e-06]]\n",
      "iteration 4\n",
      "Loss: [4.0156801]\n",
      "gradient [[-0.00416535]\n",
      " [-0.08845799]\n",
      " [-0.04706721]\n",
      " [-0.09204062]]\n",
      "gradient check: [[-6.07583483e-09]\n",
      " [-2.73937679e-06]\n",
      " [-7.75562530e-07]\n",
      " [-2.96576035e-06]]\n"
     ]
    }
   ],
   "source": [
    "def grad_U_o(u_o, v_c):\n",
    "    return -(1 - sigmoid(u_o.T @ v_c))*v_c\n",
    "\n",
    "u_o = np.random.random((4,1))\n",
    "u = np.random.random((4,3))\n",
    "v_c = np.random.random((4,1))\n",
    "h = np.diag([0.0001]*4)\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(u_o,u,v_c)\n",
    "    dj = grad_U_o(u_o,v_c).reshape((4,1))\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j)\n",
    "    print(\"gradient\",dj)\n",
    "    print(\"gradient check:\", dj - np.array([(J(u_o+h[0].reshape((-1,1)),u,v_c)-j)/np.sum(h[0]),\n",
    "                                            (J(u_o+h[1].reshape((-1,1)),u,v_c)-j)/np.sum(h[1]),\n",
    "                                            (J(u_o+h[2].reshape((-1,1)),u,v_c)-j)/np.sum(h[2]),\n",
    "                                            (J(u_o+h[3].reshape((-1,1)),u,v_c)-j)/np.sum(h[3])]))\n",
    "    u_o = u_o - dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: [4.07665146]\n",
      "gradient [[0.10957902 0.10383585 0.1062522 ]\n",
      " [0.20609141 0.19528992 0.19983448]\n",
      " [0.68047203 0.64480769 0.65981291]\n",
      " [0.24770133 0.23471901 0.24018112]]\n",
      "gradient check: [[-2.22327617e-07 -2.40490840e-07 -2.33253835e-07]\n",
      " [-7.86431977e-07 -8.50683979e-07 -8.25073774e-07]\n",
      " [-8.57345177e-06 -9.27395184e-06 -8.99472842e-06]\n",
      " [-1.13604805e-06 -1.22886904e-06 -1.19186639e-06]]\n",
      "iteration 1\n",
      "Loss: [2.63410449]\n",
      "gradient [[0.08256756 0.07716268 0.07937657]\n",
      " [0.15528945 0.14512418 0.14928797]\n",
      " [0.51273428 0.4791706  0.49291861]\n",
      " [0.18664244 0.1744248  0.17942926]]\n",
      "gradient check: [[-2.79034245e-07 -2.81624731e-07 -2.80917147e-07]\n",
      " [-9.87025978e-07 -9.96178771e-07 -9.93673004e-07]\n",
      " [-1.07603836e-05 -1.08601645e-05 -1.08329183e-05]\n",
      " [-1.42582273e-06 -1.43903550e-06 -1.43542871e-06]]\n",
      "iteration 2\n",
      "Loss: [1.8387801]\n",
      "gradient [[0.06034834 0.05658271 0.0581089 ]\n",
      " [0.11350052 0.10641828 0.10928868]\n",
      " [0.37475569 0.35137159 0.36084905]\n",
      " [0.13641631 0.12790417 0.1313541 ]]\n",
      "gradient check: [[-2.70992603e-07 -2.64735660e-07 -2.67441876e-07]\n",
      " [-9.58569129e-07 -9.36437337e-07 -9.46011409e-07]\n",
      " [-1.04502308e-05 -1.02089949e-05 -1.03133588e-05]\n",
      " [-1.38471643e-06 -1.35274998e-06 -1.36657645e-06]]\n",
      "iteration 3\n",
      "Loss: [1.40734352]\n",
      "gradient [[0.04545443 0.04301341 0.04400259]\n",
      " [0.0854887  0.08089774 0.08275814]\n",
      " [0.28226635 0.26710793 0.27325058]\n",
      " [0.10274889 0.09723102 0.09946703]]\n",
      "gradient check: [[-2.37963140e-07 -2.30431663e-07 -2.33554405e-07]\n",
      " [-8.41731652e-07 -8.15099332e-07 -8.26144368e-07]\n",
      " [-9.17651842e-06 -8.88617427e-06 -9.00659960e-06]\n",
      " [-1.21593732e-06 -1.17746374e-06 -1.19341986e-06]]\n",
      "iteration 4\n",
      "Loss: [1.15559112]\n",
      "gradient [[0.03574323 0.03411976 0.03477944]\n",
      " [0.0672243  0.06417095 0.06541164]\n",
      " [0.221961   0.21187947 0.21597598]\n",
      " [0.0807969  0.07712709 0.07861827]]\n",
      "gradient check: [[-2.04479533e-07 -1.97962083e-07 -2.00643237e-07]\n",
      " [-7.23289910e-07 -7.00234741e-07 -7.09716026e-07]\n",
      " [-7.88529881e-06 -7.63395453e-06 -7.73730935e-06]\n",
      " [-1.04483947e-06 -1.01153462e-06 -1.02523156e-06]]\n"
     ]
    }
   ],
   "source": [
    "def grad_U(u, v_c):\n",
    "    return v_c @ (1 - sigmoid(-u.T @ v_c)).T\n",
    "\n",
    "u_o = np.random.random((4,1))\n",
    "u = np.random.random((4,3))\n",
    "v_c = np.random.random((4,1))\n",
    "h = np.zeros((12,4,3))\n",
    "for i in range(12):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    h[i,row,col] = 0.0001\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(u_o,u,v_c)\n",
    "    dj = grad_U(u,v_c)\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j)\n",
    "    print(\"gradient\",dj)\n",
    "    diff = np.zeros_like(u)\n",
    "    for i in range(12):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        diff[row,col] = (J(u_o,u+h[i],v_c)-j)/np.sum(h[i])\n",
    "    print(\"gradient check:\", dj - diff)\n",
    "    u = u - dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: [4.72987672]\n",
      "gradient [[1.1069796 ]\n",
      " [0.8000436 ]\n",
      " [1.35019401]\n",
      " [1.2601191 ]]\n",
      "gradient check: [[-1.27866749e-05]\n",
      " [-6.36308131e-06]\n",
      " [-1.07693757e-05]\n",
      " [-1.15491254e-05]]\n",
      "iteration 1\n",
      "Loss: [1.99581972]\n",
      "gradient [[-0.14393218]\n",
      " [-0.07702408]\n",
      " [ 0.22377566]\n",
      " [ 0.13736781]]\n",
      "gradient check: [[-1.18785756e-05]\n",
      " [-5.98597260e-06]\n",
      " [-9.90202495e-06]\n",
      " [-1.08015012e-05]]\n",
      "iteration 2\n",
      "Loss: [1.90321053]\n",
      "gradient [[-0.16445899]\n",
      " [-0.0934769 ]\n",
      " [ 0.1950267 ]\n",
      " [ 0.11039826]]\n",
      "gradient check: [[-1.13935756e-05]\n",
      " [-5.76798298e-06]\n",
      " [-9.30438815e-06]\n",
      " [-1.02310958e-05]]\n",
      "iteration 3\n",
      "Loss: [1.81862326]\n",
      "gradient [[-0.16654173]\n",
      " [-0.09649856]\n",
      " [ 0.18279018]\n",
      " [ 0.1006706 ]]\n",
      "gradient check: [[-1.12929319e-05]\n",
      " [-5.71486405e-06]\n",
      " [-9.04350038e-06]\n",
      " [-9.97925962e-06]]\n",
      "iteration 4\n",
      "Loss: [1.73924734]\n",
      "gradient [[-0.16327516]\n",
      " [-0.09556005]\n",
      " [ 0.17547567]\n",
      " [ 0.0960344 ]]\n",
      "gradient check: [[-1.12928979e-05]\n",
      " [-5.70480407e-06]\n",
      " [-8.87860299e-06]\n",
      " [-9.81921726e-06]]\n"
     ]
    }
   ],
   "source": [
    "def grad_v_c(u_o, u, v_c):\n",
    "    return -(1 - sigmoid(u_o.T @ v_c))*u_o + u @ (1 - sigmoid(-u.T @ v_c))\n",
    "\n",
    "u_o = np.random.random((4,1))\n",
    "u = np.random.random((4,3))\n",
    "v_c = np.random.random((4,1))\n",
    "h = np.diag([0.0001]*4)\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(u_o,u,v_c)\n",
    "    dj = grad_v_c(u_o, u, v_c)\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j)\n",
    "    print(\"gradient\",dj)\n",
    "    print(\"gradient check:\", dj - np.array([(J(u_o,u,v_c+h[0].reshape((-1,1)))-j)/np.sum(h[0]),\n",
    "                                            (J(u_o,u,v_c+h[1].reshape((-1,1)))-j)/np.sum(h[1]),\n",
    "                                            (J(u_o,u,v_c+h[2].reshape((-1,1)))-j)/np.sum(h[2]),\n",
    "                                            (J(u_o,u,v_c+h[3].reshape((-1,1)))-j)/np.sum(h[3])]))\n",
    "    v_c = v_c - dj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__f) Suppose the center word is $c = w_t$ and the context window is $[w_{t−m}, ..., w_{t−1}, w_t, w_{t+1}, ...,\n",
    "w_{t+m}]$, where $m$ is the context window size. Recall that for the skip-gram version of word2vec, the total loss for the context window is:__\n",
    "\n",
    "$$J_{\\text skip-gram}(v_c, w_{t−m}, ..., w_{t+m}, U) = \\sum_{-m \\le j \\le m} {J(v_c, w_{t+j}, U)}$$\n",
    "\n",
    "__Here, $J(v_c, w_{t+j}, U)$ represents an arbitrary loss term for the center word $c = w_t$ and outside word $w_{t+j}$. $J(v_c, w_{t+j}, U)$ could be $J_{\\text naive-softmax}(v_c, w_{t+j}, U)$ or $J_{\\text neg-sample}(v_c, w_{t+j}, U)$, depending on your implementation.__\n",
    "\n",
    "__Write down three partial derivatives:__\n",
    "- $\\frac {\\partial J_{\\text skip-gram}} {\\partial U}$\n",
    "- $\\frac {\\partial J_{\\text skip-gram}} {\\partial v_c}$\n",
    "- $\\frac {\\partial J_{\\text skip-gram}} {\\partial v_w} \\text {, where } w \\ne c$\n",
    "\n",
    "__Write your answers in terms of $\\frac {\\partial {J(v_c, w_{t+j}, U)}} {\\partial U}$ and $\\frac {\\partial {J(v_c, w_{t+j}, U)}} {\\partial v_c}$__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$$\\frac {\\partial J_{\\text skip-gram}} {\\partial U} = \\sum_{-m \\le j \\le m} {\\frac {\\partial {J(v_c, w_{t+j}, U)}} {\\partial U}}$$\n",
    "\n",
    "$$\\frac {\\partial J_{\\text skip-gram}} {\\partial v_c} = \\sum_{-m \\le j \\le m} {\\frac {\\partial {J(v_c, w_{t+j}, U)}} {\\partial v_c}}$$\n",
    "\n",
    "$$\\frac {\\partial J_{\\text skip-gram}} {\\partial v_w} = 0 \\text {, for all } w \\ne c $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding: Implementing word2vec\n",
    "__After 40,000 iterations, the script will finish and a visualization for your word vectors will appear. It will\n",
    "also be saved as word vectors.png in your project directory. Include the plot in your homework\n",
    "write up. Briefly explain in at most three sentences what you see in the plot.__\n",
    "\n",
    "Result:\n",
    "![SVD visualization of trained word2vec model](word_vectors.png)\n",
    "\n",
    "We can see that words with opposite meanings placed far away from each other. Moreover if the difference in meaning is similar between two pairs of words, then we can see similar shift in terms of X and Y (in other words difference between two word-vectors is similar between this two pairs). For example we can draw a vector from \"male\" to \"female\" and it will be close to vector between \"king\" and \"queen\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
