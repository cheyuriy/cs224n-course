{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__a) Show that the naive-softmax loss given in $J_{\\text {naive_softmax}}(v_c, o, U) = - \\log P(O=o|C=c)$ is the same as the cross-entropy loss between $y$ and $\\hat{y}$; i.e., show that:__\n",
    "\n",
    "$$- \\sum \\limits_{w \\in \\text {Vocab}} {y_w \\log \\hat {y}_w} = - \\log \\hat {y}_o$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "$y_w$ is non zero and equal to 1 only in the position $k$ of true word $o$. So left part can be simplified and be written as the only non zero term of a sum: $- \\sum \\limits_{w \\in \\text {Vocab}} {y_w \\log \\hat {y}_w} = - \\sum \\limits_{w = o} {y_w \\log \\hat {y}_w} = - y_o \\log \\hat{y}_o = - 1 * \\log \\hat{y}_o = - \\log \\hat{y}_o$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__b) Compute the partial derivative of $J_{\\text {naive-softmax}}(v_c, o, U)$ with respect to $v_c$ . Please write your answer in terms of $y$, $\\hat{y}$, and $U$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "$\\frac {\\partial J} {\\partial v_c} = (y - \\hat{y})U^T$\n",
    "\n",
    "Numerical check (loss should be decreasing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: 1.6286669460358003 v_c [0.11044461 0.49637592 0.59164704 0.52777441]\n",
      "gradient [ 0.12555521  0.06494604 -0.05090479  0.0246106 ]\n",
      "gradient check: [-1.79127658e-07 -4.06441447e-07 -7.98708971e-08 -3.36656124e-07]\n",
      "iteration 1\n",
      "Loss: 1.6056614757829795 v_c [-0.0151106   0.43142988  0.64255183  0.50316381]\n",
      "gradient [ 0.12319114  0.06573569 -0.05017703  0.02198738]\n",
      "gradient check: [-1.75666377e-07 -4.01828840e-07 -8.04450533e-08 -3.40770149e-07]\n",
      "iteration 2\n",
      "Loss: 1.5833234184915304 v_c [-0.13830174  0.36569419  0.69272886  0.48117643]\n",
      "gradient [ 0.12099035  0.06627528 -0.04953588  0.01958544]\n",
      "gradient check: [-1.72309719e-07 -3.97353032e-07 -8.10396256e-08 -3.44556652e-07]\n",
      "iteration 3\n",
      "Loss: 1.561604196838108 v_c [-0.25929209  0.2994189   0.74226474  0.46159098]\n",
      "gradient [ 0.11893103  0.06660231 -0.04897301  0.01738821]\n",
      "gradient check: [-1.69131233e-07 -3.92990247e-07 -8.15579603e-08 -3.47884898e-07]\n",
      "iteration 4\n",
      "Loss: 1.5404628684279995 v_c [-0.37822312  0.23281659  0.79123775  0.44420277]\n",
      "gradient [ 0.11699474  0.06674882 -0.04848062  0.01537972]\n",
      "gradient check: [-1.66070821e-07 -3.88712235e-07 -8.20661653e-08 -3.50885472e-07]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def J(y, U, v_c):\n",
    "    #softmax value for each word in vocabulary. Array shape [1,V], where V - size of vocabulary\n",
    "    y_hat = np.exp(U.T @ v_c) / np.sum(np.exp(U.T @ v_c))\n",
    "    #loss function. Scalar value.\n",
    "    J = np.sum(- y * np.log(y_hat))\n",
    "    return J\n",
    "\n",
    "def grad_v_c(y, U, v_c):\n",
    "    y_hat = np.exp(U.T @ v_c) / np.sum(np.exp(U.T @ v_c))\n",
    "    #sum over rows. so we get shape [1,d], where d is the size of our vector representation\n",
    "    d = np.sum((y_hat-y)[:,np.newaxis]*U.T, axis=0)\n",
    "    return d\n",
    "\n",
    "#some random matrix U (suppose d=4, v=5, so shape is 4x5)\n",
    "U = np.array(np.random.random((4,5)))\n",
    "\n",
    "#some random word vector V_c\n",
    "v_c = np.random.random(4)\n",
    "\n",
    "#some random vector y with correct labels of words from vocabulary (size v, one-hot encoded)\n",
    "y = np.array([0,0,1,0,0])\n",
    "\n",
    "#matrix with small shift for every element in word_vector v_c to perform finite differences check\n",
    "h = np.diag([0.00001]*4)\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(y,U,v_c)\n",
    "    dj = grad_v_c(y,U,v_c)\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j,\"v_c\",v_c)\n",
    "    print(\"gradient\",dj)\n",
    "    print(\"gradient check:\", dj - [(J(y,U,v_c+h[0])-j)/np.sum(h[0]),\n",
    "                                   (J(y,U,v_c+h[1])-j)/np.sum(h[1]),\n",
    "                                   (J(y,U,v_c+h[2])-j)/np.sum(h[2]),\n",
    "                                   (J(y,U,v_c+h[3])-j)/np.sum(h[3])])\n",
    "    v_c = v_c - dj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__c) Compute the partial derivatives of $J_{\\text naive-softmax} (v_c, o, U)$ with respect to each of the ‘outside’\n",
    "word vectors, $u_w$ ’s. There will be two cases: when $w = o$, the true ‘outside’ word vector, and $w \\ne o$, for\n",
    "all other words. Please write you answer in terms of $y$, $\\hat{y}$, and $v_c$.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$$\\frac {\\partial J} {\\partial U} = v_c (y - \\hat{y})^T$$ \n",
    "\n",
    "Numerical check (loss should be decreasing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: 1.6425340884085566 v_c [0.32366551 0.78602271 0.95299673 0.32574079]\n",
      "gradient [[ 0.07071273  0.06883717 -0.26103976  0.08263298  0.03885688]\n",
      " [ 0.17172609  0.16717129 -0.63393588  0.20067446  0.09436405]\n",
      " [ 0.20820568  0.20268332 -0.76860225  0.24330353  0.11440971]\n",
      " [ 0.07116612  0.06927854 -0.2627135   0.08316281  0.03910602]]\n",
      "gradient check: [[-8.94299828e-08 -8.77025546e-08 -8.17395531e-08 -9.95782286e-08\n",
      "  -5.53298021e-08]\n",
      " [-5.27448146e-07 -5.17278156e-07 -4.82059162e-07 -5.87294578e-07\n",
      "  -3.26345596e-07]\n",
      " [-7.75350542e-07 -7.60388967e-07 -7.08630746e-07 -8.63342035e-07\n",
      "  -4.79706960e-07]\n",
      " [-9.05928829e-08 -8.88221932e-08 -8.27795587e-08 -1.00861391e-07\n",
      "  -5.60309616e-08]]\n",
      "iteration 1\n",
      "Loss: 0.5366733735791156 v_c [0.32366551 0.78602271 0.95299673 0.32574079]\n",
      "gradient [[ 0.03602409  0.03542334 -0.1344215   0.03948824  0.02348582]\n",
      " [ 0.08748462  0.08602571 -0.32644304  0.09589733  0.05703539]\n",
      " [ 0.10606889  0.10430006 -0.39578901  0.1162687   0.06915136]\n",
      " [ 0.03625507  0.03565047 -0.13528339  0.03974144  0.02363641]]\n",
      "gradient check: [[-5.18134230e-08 -5.10510974e-08 -1.27181244e-07 -5.61162046e-08\n",
      "  -3.52475964e-08]\n",
      " [-3.05542863e-07 -3.01084364e-07 -7.50144927e-07 -3.30916336e-07\n",
      "  -2.07906303e-07]\n",
      " [-4.49150286e-07 -4.42593877e-07 -1.10267514e-06 -4.86420768e-07\n",
      "  -3.05597676e-07]\n",
      " [-5.24833037e-08 -5.17000714e-08 -1.28827970e-07 -5.68360087e-08\n",
      "  -3.57033441e-08]]\n",
      "iteration 2\n",
      "Loss: 0.2521173699835465 v_c [0.32366551 0.78602271 0.95299673 0.32574079]\n",
      "gradient [[ 0.01918395  0.01892495 -0.07212772  0.02064141  0.01337741]\n",
      " [ 0.04658828  0.04595929 -0.1751624   0.05012773  0.03248709]\n",
      " [ 0.05648499  0.05572238 -0.21237197  0.06077631  0.03938829]\n",
      " [ 0.01930695  0.01904629 -0.07259019  0.02077376  0.01346319]]\n",
      "gradient check: [[-2.92272059e-08 -2.88433307e-08 -9.07117983e-08 -3.12751177e-08\n",
      "  -2.07541368e-08]\n",
      " [-1.72259268e-07 -1.70063101e-07 -5.35001719e-07 -1.84434796e-07\n",
      "  -1.22410545e-07]\n",
      " [-2.53196240e-07 -2.50000634e-07 -7.86444494e-07 -2.71127443e-07\n",
      "  -1.79938767e-07]\n",
      " [-2.95965353e-08 -2.92052448e-08 -9.18821667e-08 -3.16718734e-08\n",
      "  -2.10259358e-08]]\n",
      "iteration 3\n",
      "Loss: 0.16243402868913243 v_c [0.32366551 0.78602271 0.95299673 0.32574079]\n",
      "gradient [[ 0.01285519  0.01269927 -0.04852647  0.01372407  0.00924794]\n",
      " [ 0.03121886  0.03084021 -0.11784668  0.03332895  0.02245865]\n",
      " [ 0.03785065  0.03739156 -0.14288073  0.04040899  0.02722952]\n",
      " [ 0.01293761  0.01278069 -0.04883761  0.01381207  0.00930724]]\n",
      "gradient check: [[-1.99867358e-08 -1.97383898e-08 -6.67503008e-08 -2.12629075e-08\n",
      "  -1.45450734e-08]\n",
      " [-1.17826639e-07 -1.16450356e-07 -3.93701177e-07 -1.25427765e-07\n",
      "  -8.57367073e-08]\n",
      " [-1.73192902e-07 -1.71182701e-07 -5.78741296e-07 -1.84374734e-07\n",
      "  -1.26035536e-07]\n",
      " [-2.02316176e-08 -1.99958856e-08 -6.76210887e-08 -2.15326030e-08\n",
      "  -1.47257536e-08]]\n",
      "iteration 4\n",
      "Loss: 0.11974314764627159 v_c [0.32366551 0.78602271 0.95299673 0.32574079]\n",
      "gradient [[ 0.00965082  0.00954175 -0.03652621  0.01025519  0.00707845]\n",
      " [ 0.02343705  0.02317217 -0.08870402  0.02490477  0.01719003]\n",
      " [ 0.02841576  0.02809461 -0.10754733  0.03019527  0.02084169]\n",
      " [ 0.0097127   0.00960293 -0.03676041  0.01032095  0.00712383]]\n",
      "gradient check: [[-1.51603949e-08 -1.49799134e-08 -5.24469277e-08 -1.60650489e-08\n",
      "  -1.12098345e-08]\n",
      " [-8.93705160e-08 -8.83963298e-08 -3.09282017e-07 -9.47854731e-08\n",
      "  -6.60876582e-08]\n",
      " [-1.31373677e-07 -1.29934451e-07 -4.54631327e-07 -1.39316344e-07\n",
      "  -9.71504106e-08]\n",
      " [-1.53434935e-08 -1.51771298e-08 -5.31131351e-08 -1.62850806e-08\n",
      "  -1.13518758e-08]]\n"
     ]
    }
   ],
   "source": [
    "def grad_U(y, U, v_c):\n",
    "    y_hat = np.exp(U.T @ v_c) / np.sum(np.exp(U.T @ v_c))\n",
    "    #sum over rows. so we get shape [1,d], where d is the size of our vector representation\n",
    "    d = v_c.T[:,np.newaxis] @ (y_hat-y).T[np.newaxis,:]\n",
    "    return d\n",
    "\n",
    "#some random matrix U (suppose d=4, v=5, so shape is 4x5)\n",
    "U = np.array(np.random.random((4,5)))\n",
    "\n",
    "#some random word vector V_c\n",
    "v_c = np.random.random(4)\n",
    "\n",
    "#some random vector y with correct labels of words from vocabulary (size v, one-hot encoded)\n",
    "y = np.array([0,0,1,0,0])\n",
    "\n",
    "#matrix with small shift for every element in word_vector v_c to perform finite differences check\n",
    "h = np.zeros((20,4,5))\n",
    "for i in range(20):\n",
    "    row = i // 5\n",
    "    col = i % 5\n",
    "    h[i,row,col] = 0.00001\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(y,U,v_c)\n",
    "    dj = grad_U(y,U,v_c)\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j,\"v_c\",v_c)\n",
    "    print(\"gradient\",dj)\n",
    "    diff = np.zeros_like(U)\n",
    "    for i in range(20):\n",
    "        row = i // 5\n",
    "        col = i % 5\n",
    "        diff[row,col] = (J(y,U+h[i],v_c)-j)/np.sum(h[i])\n",
    "    print(\"gradient check:\", dj - diff)\n",
    "    U = U - dj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__d) The sigmoid function is given by equation:\n",
    "$$ \\sigma(x) = \\frac 1 {1 + e^{-x}} = \\frac {e^x} {e^x + 1} $$\n",
    "Please compute the derivative of $\\sigma(x)$ with respect to $x$, where $x$ is a vector.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$$\\sigma^\\prime(x) = \\frac {(e^x)^\\prime (e^x + 1) - e^x (e^x + 1)^\\prime} {(e^x + 1)^2} = \\frac {e^x (e^x + 1) - e^x e^x} {(e^x + 1)^2} = \\frac {e^x} {(e^x + 1)^2} = \\frac {e^x} {e^x + 1} \\frac 1 {e^x + 1} = \\sigma(x) (1 - \\frac {e^x} {e^x + 1}) = \\sigma(x) (1 - \\sigma(x))$$\n",
    "\n",
    "Numerical check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X [0.6838126  0.52530528 0.87615279 0.56177687]\n",
      "Sigmoid value: [0.6645891  0.62838748 0.70602435 0.63686357]\n",
      "Gradient for x [0.22291043 0.23351666 0.20755397 0.23126836]\n",
      "gradient check: [3.66898801e-06 2.99821760e-06 4.27620167e-06 3.16537091e-06]\n",
      "X [0.79460562 0.37616876 0.76823168 0.07692841]\n",
      "Sigmoid value: [0.68881939 0.59294872 0.68313825 0.51922262]\n",
      "Gradient for x [0.21434724 0.24136053 0.21646038 0.24963049]\n",
      "gradient check: [4.04739409e-06 2.24359611e-06 3.96432437e-06 4.80063570e-07]\n",
      "X [0.43845801 0.30115655 0.61410629 0.55144119]\n",
      "Sigmoid value: [0.60789154 0.57472522 0.64887693 0.63446989]\n",
      "Gradient for x [0.23835941 0.24441614 0.22783566 0.23191785]\n",
      "gradient check: [2.57186682e-06 1.82659335e-06 3.39208660e-06 3.11874846e-06]\n",
      "X [0.32132219 0.99601131 0.13700658 0.51930419]\n",
      "Sigmoid value: [0.57964645 0.73027363 0.53419817 0.62698505]\n",
      "Gradient for x [0.24365644 0.19697405 0.24883049 0.2338748 ]\n",
      "gradient check: [1.94082456e-06 4.53585153e-06 8.51158838e-07 2.97001772e-06]\n",
      "X [0.99899037 0.15923505 0.72070481 0.97158773]\n",
      "Sigmoid value: [0.73086003 0.53972486 0.6727622  0.72543585]\n",
      "Gradient for x [0.19670365 0.24842194 0.22015322 0.19917868]\n",
      "gradient check: [4.54116019e-06 9.87056915e-07 3.80353237e-06 4.49026598e-06]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    #naive sigmoid implementation\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def grad_sigmoid(x):\n",
    "    return sigmoid(x)*(1 - sigmoid(x))\n",
    "\n",
    "#matrix with small shift for every element in vector X to perform finite differences check\n",
    "h = np.array([0.0001]*4)\n",
    "\n",
    "for i in range(5):\n",
    "    #some random vector X (suppose d=4)\n",
    "    X = np.random.random(4)\n",
    "    s = sigmoid(X)\n",
    "    ds = grad_sigmoid(X)\n",
    "    print(\"X\",X)\n",
    "    print(\"Sigmoid value:\",s)\n",
    "    print(\"Gradient for x\",ds)\n",
    "    print(\"gradient check:\", ds - (sigmoid(X+h)-s)/h[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__e) Now we shall consider the Negative Sampling loss, which is an alternative to the Naive Softmax loss. Assume that $K$ negative samples (words) are drawn from the vocabulary. For simplicity of notation we shall refer to them as $w_1$, $w_2$, ..., $w_K$ and their outside vectors as $u_1$, ..., $u_K$. Note that $o \\notin \\{w_1, ..., w_K\\}$. For a center word $c$ and an outside word $o$, the negative sampling loss function is given by:__\n",
    "\n",
    "$$J_{\\text neg-sample}(v_c, o, U) = − \\log(\\sigma(u_o v_c)) − \\sum\\limits_K {\\log(\\sigma(−u_k v_c))}$$\n",
    "\n",
    "__for a sample $w_1$, ..., $w_K$, where $\\sigma(x)$ is the sigmoid function. Please repeat parts (b) and (c), computing the partial derivatives of $J_{\\text neg-sample}$ with respect to $v_c$, with respect to $u_o$, and with respect to a negative sample $u_k$. Please write your answers in terms of the vectors $u_o$, $v_c$ and $u_k$, where $k \\in [1,K]$.__\n",
    "\n",
    "__After you’ve done this, describe with one sentence why this loss function is much more efficient to compute than the naive-softmax loss. Note, you should be able to use your solution to part (d) to help compute the necessary gradients here.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$$\\frac {\\partial{J}} {\\partial{u_o}} = - (1 - \\sigma(u_o^T v_c))v_c$$\n",
    "\n",
    "$$\\frac {\\partial{J}} {\\partial{u_k}} = (1 - \\sigma(-u_k^T v_c))v_c \\\\ \n",
    "\\text{or in matrix form} \\\\ \n",
    "\\frac {\\partial{J}} {\\partial{U}} = v_c \\times (1 - \\sigma(-U^T v_c))^T$$\n",
    "\n",
    "$$\\frac {\\partial{J}} {\\partial{v_c}} = - (1 - \\sigma(u_o^T v_c)) u_o + \\sum\\limits_K {(1 - \\sigma(-u_k^T v_c))u_k} \\\\ \n",
    "\\text{or in matrix form} \\\\ \n",
    "\\frac {\\partial{J}} {\\partial{v_c}} = - (1 - \\sigma(u_o^T v_c)) u_o + U \\times (1 - \\sigma(-U^T v_c))$$\n",
    "\n",
    "Numerical check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def J(u_o, u, v_c):\n",
    "    J = -np.log(sigmoid(u_o.T @ v_c)) - np.sum(np.log(sigmoid(-u.T @ v_c)), axis = 0)\n",
    "    return J.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: [4.25447771]\n",
      "gradient [[-0.13942545]\n",
      " [-0.05123947]\n",
      " [-0.02688636]\n",
      " [-0.15435727]]\n",
      "gradient check: [[-5.06282006e-06]\n",
      " [-6.83794527e-07]\n",
      " [-1.88272380e-07]\n",
      " [-6.20528501e-06]]\n",
      "iteration 1\n",
      "Loss: [4.2131611]\n",
      "gradient [[-0.10879054]\n",
      " [-0.039981  ]\n",
      " [-0.02097883]\n",
      " [-0.12044151]]\n",
      "gradient check: [[-4.11703830e-06]\n",
      " [-5.56052748e-07]\n",
      " [-1.53097848e-07]\n",
      " [-5.04607738e-06]]\n",
      "iteration 2\n",
      "Loss: [4.18742906]\n",
      "gradient [[-0.08906147]\n",
      " [-0.03273048]\n",
      " [-0.01717433]\n",
      " [-0.09859954]]\n",
      "gradient check: [[-3.45826633e-06]\n",
      " [-4.67082714e-07]\n",
      " [-1.28601735e-07]\n",
      " [-4.23864607e-06]]\n",
      "iteration 3\n",
      "Loss: [4.16991111]\n",
      "gradient [[-0.07533683]\n",
      " [-0.02768662]\n",
      " [-0.01452772]\n",
      " [-0.08340505]]\n",
      "gradient check: [[-2.97702822e-06]\n",
      " [-4.02076851e-07]\n",
      " [-1.10709814e-07]\n",
      " [-3.64881489e-06]]\n",
      "iteration 4\n",
      "Loss: [4.15723224]\n",
      "gradient [[-0.06525236]\n",
      " [-0.02398053]\n",
      " [-0.01258306]\n",
      " [-0.07224058]]\n",
      "gradient check: [[-2.61142928e-06]\n",
      " [-3.52704106e-07]\n",
      " [-9.71098873e-08]\n",
      " [-3.20071715e-06]]\n"
     ]
    }
   ],
   "source": [
    "def grad_U_o(u_o, v_c):\n",
    "    return -(1 - sigmoid(u_o.T @ v_c))*v_c\n",
    "\n",
    "u_o = np.random.random((4,1))\n",
    "u = np.random.random((4,3))\n",
    "v_c = np.random.random((4,1))\n",
    "h = np.diag([0.0001]*4)\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(u_o,u,v_c)\n",
    "    dj = grad_U_o(u_o,v_c).reshape((4,1))\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j)\n",
    "    print(\"gradient\",dj)\n",
    "    print(\"gradient check:\", dj - np.array([(J(u_o+h[0].reshape((-1,1)),u,v_c)-j)/np.sum(h[0]),\n",
    "                                            (J(u_o+h[1].reshape((-1,1)),u,v_c)-j)/np.sum(h[1]),\n",
    "                                            (J(u_o+h[2].reshape((-1,1)),u,v_c)-j)/np.sum(h[2]),\n",
    "                                            (J(u_o+h[3].reshape((-1,1)),u,v_c)-j)/np.sum(h[3])]))\n",
    "    u_o = u_o - dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: [4.69323642]\n",
      "gradient [[0.68944767 0.58271003 0.5562555 ]\n",
      " [0.69300844 0.58571953 0.55912837]\n",
      " [0.11040547 0.09331292 0.08907659]\n",
      " [0.06534438 0.05522802 0.05272071]]\n",
      "gradient check: [[-4.61534981e-06 -7.01066395e-06 -7.42816962e-06]\n",
      " [-4.66315209e-06 -7.08327253e-06 -7.50509682e-06]\n",
      " [-1.18358924e-07 -1.79775024e-07 -1.90485117e-07]\n",
      " [-4.14665845e-08 -6.29782596e-08 -6.67234966e-08]]\n",
      "iteration 1\n",
      "Loss: [2.72423573]\n",
      "gradient [[0.50823638 0.3917862  0.37004832]\n",
      " [0.51086125 0.39380964 0.3719595 ]\n",
      " [0.081387   0.06273912 0.0592581 ]\n",
      " [0.04816956 0.03713266 0.03507239]]\n",
      "gradient check: [[-8.00720026e-06 -8.45377267e-06 -8.38693249e-06]\n",
      " [-8.09012194e-06 -8.54131747e-06 -8.47379099e-06]\n",
      " [-2.05337584e-07 -2.16786144e-07 -2.15070723e-07]\n",
      " [-7.19267686e-08 -7.59393574e-08 -7.53377815e-08]]\n",
      "iteration 2\n",
      "Loss: [1.79095642]\n",
      "gradient [[0.33484305 0.26301125 0.25070819]\n",
      " [0.3365724  0.26436961 0.25200302]\n",
      " [0.05362046 0.0421176  0.04014743]\n",
      " [0.03173571 0.02492765 0.02376159]]\n",
      "gradient check: [[-8.17845918e-06 -7.36865171e-06 -7.17819740e-06]\n",
      " [-8.26315464e-06 -7.44496597e-06 -7.25253470e-06]\n",
      " [-2.09719590e-07 -1.88956506e-07 -1.84074852e-07]\n",
      " [-7.34624301e-08 -6.61895637e-08 -6.44800502e-08]]\n",
      "iteration 3\n",
      "Loss: [1.36613017]\n",
      "gradient [[0.23103584 0.19070588 0.18363292]\n",
      " [0.23222906 0.19169081 0.18458132]\n",
      " [0.03699718 0.0305389  0.02940626]\n",
      " [0.02189709 0.0180747  0.01740434]]\n",
      "gradient check: [[-6.84220690e-06 -6.03240102e-06 -5.87361600e-06]\n",
      " [-6.91306550e-06 -6.09487377e-06 -5.93444251e-06]\n",
      " [-1.75455971e-07 -1.54690273e-07 -1.50618643e-07]\n",
      " [-6.14634197e-08 -5.41861773e-08 -5.27615682e-08]]\n",
      "iteration 4\n",
      "Loss: [1.14446912]\n",
      "gradient [[0.17214888 0.14772726 0.14329479]\n",
      " [0.17303797 0.14849022 0.14403486]\n",
      " [0.02756725 0.02365647 0.02294667]\n",
      " [0.0163159  0.01400127 0.01358118]]\n",
      "gradient check: [[-5.60514527e-06 -4.99037720e-06 -4.87240437e-06]\n",
      " [-5.66319576e-06 -5.04205954e-06 -4.92286273e-06]\n",
      " [-1.43736037e-07 -1.27969561e-07 -1.24946275e-07]\n",
      " [-5.03533947e-08 -4.48268736e-08 -4.37689210e-08]]\n"
     ]
    }
   ],
   "source": [
    "def grad_U(u, v_c):\n",
    "    return v_c @ (1 - sigmoid(-u.T @ v_c)).T\n",
    "\n",
    "u_o = np.random.random((4,1))\n",
    "u = np.random.random((4,3))\n",
    "v_c = np.random.random((4,1))\n",
    "h = np.zeros((12,4,3))\n",
    "for i in range(12):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    h[i,row,col] = 0.0001\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(u_o,u,v_c)\n",
    "    dj = grad_U(u,v_c)\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j)\n",
    "    print(\"gradient\",dj)\n",
    "    diff = np.zeros_like(u)\n",
    "    for i in range(12):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        diff[row,col] = (J(u_o,u+h[i],v_c)-j)/np.sum(h[i])\n",
    "    print(\"gradient check:\", dj - diff)\n",
    "    u = u - dj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "Loss: [4.74420864]\n",
      "gradient [[1.44004953]\n",
      " [0.83595838]\n",
      " [1.0429235 ]\n",
      " [0.73707864]]\n",
      "gradient check: [[-1.28144872e-05]\n",
      " [-6.10326807e-06]\n",
      " [-9.30622945e-06]\n",
      " [-5.57731323e-06]]\n",
      "iteration 1\n",
      "Loss: [1.98857823]\n",
      "gradient [[0.45151339]\n",
      " [0.13523654]\n",
      " [0.1867848 ]\n",
      " [0.12469523]]\n",
      "gradient check: [[-1.44567119e-05]\n",
      " [-6.63057893e-06]\n",
      " [-1.03890310e-05]\n",
      " [-5.86220130e-06]]\n",
      "iteration 2\n",
      "Loss: [1.78802359]\n",
      "gradient [[0.27016981]\n",
      " [0.01593348]\n",
      " [0.04689248]\n",
      " [0.01601042]]\n",
      "gradient check: [[-1.11459844e-05]\n",
      " [-5.08473339e-06]\n",
      " [-7.55511551e-06]\n",
      " [-4.76154026e-06]]\n",
      "iteration 3\n",
      "Loss: [1.72307934]\n",
      "gradient [[ 0.20443395]\n",
      " [-0.02302179]\n",
      " [ 0.00313705]\n",
      " [-0.02083734]]\n",
      "gradient check: [[-9.70767802e-06]\n",
      " [-4.50838810e-06]\n",
      " [-6.51477863e-06]\n",
      " [-4.33537461e-06]]\n",
      "iteration 4\n",
      "Loss: [1.68348328]\n",
      "gradient [[ 0.17047353]\n",
      " [-0.04045148]\n",
      " [-0.01571637]\n",
      " [-0.03772361]]\n",
      "gradient check: [[-8.89888078e-06]\n",
      " [-4.24235167e-06]\n",
      " [-6.02978694e-06]\n",
      " [-4.13987573e-06]]\n"
     ]
    }
   ],
   "source": [
    "def grad_v_c(u_o, u, v_c):\n",
    "    return -(1 - sigmoid(u_o.T @ v_c))*u_o + u @ (1 - sigmoid(-u.T @ v_c))\n",
    "\n",
    "u_o = np.random.random((4,1))\n",
    "u = np.random.random((4,3))\n",
    "v_c = np.random.random((4,1))\n",
    "h = np.diag([0.0001]*4)\n",
    "\n",
    "for i in range(5):\n",
    "    j = J(u_o,u,v_c)\n",
    "    dj = grad_v_c(u_o, u, v_c)\n",
    "    print(\"iteration\",i)\n",
    "    print(\"Loss:\",j)\n",
    "    print(\"gradient\",dj)\n",
    "    print(\"gradient check:\", dj - np.array([(J(u_o,u,v_c+h[0].reshape((-1,1)))-j)/np.sum(h[0]),\n",
    "                                            (J(u_o,u,v_c+h[1].reshape((-1,1)))-j)/np.sum(h[1]),\n",
    "                                            (J(u_o,u,v_c+h[2].reshape((-1,1)))-j)/np.sum(h[2]),\n",
    "                                            (J(u_o,u,v_c+h[3].reshape((-1,1)))-j)/np.sum(h[3])]))\n",
    "    v_c = v_c - dj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__f) Suppose the center word is $c = w_t$ and the context window is $[w_{t−m}, ..., w_{t−1}, w_t, w_{t+1}, ...,\n",
    "w_{t+m}]$, where $m$ is the context window size. Recall that for the skip-gram version of word2vec, the total loss for the context window is:__\n",
    "\n",
    "$$J_{\\text skip-gram}(v_c, w_{t−m}, ..., w_{t+m}, U) = \\sum\\limits_{-m \\le j \\le m} {J(v_c, w_{t+j}, U)}$$\n",
    "\n",
    "__Here, $J(v_c, w_{t+j}, U)$ represents an arbitrary loss term for the center word $c = w_t$ and outside word $w_{t+j}$. $J(v_c, w_{t+j}, U)$ could be $J_{\\text naive-softmax}(v_c, w_{t+j}, U)$ or $J_{\\text neg-sample}(v_c, w_{t+j}, U)$, depending on your implementation.__\n",
    "\n",
    "__Write down three partial derivatives:__\n",
    "- $\\frac {\\partial J_{\\text skip-gram}} {\\partial U}$\n",
    "- $\\frac {\\partial J_{\\text skip-gram}} {\\partial v_c}$\n",
    "- $\\frac {\\partial J_{\\text skip-gram}} {\\partial v_w} \\text {, where } w \\ne c$\n",
    "\n",
    "__Write your answers in terms of $\\frac {\\partial {J(v_c, w_{t+j}, U)}} {\\partial U}$ and $\\frac {\\partial {J(v_c, w_{t+j}, U)}} {\\partial v_c}$__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n",
    "\n",
    "$$\\frac {\\partial J_{\\text skip-gram}} {\\partial U} = \\sum\\limits_{-m \\le j \\le m} {\\frac {\\partial {J(v_c, w_{t+j}, U)}} {\\partial U}}$$\n",
    "\n",
    "$$\\frac {\\partial J_{\\text skip-gram}} {\\partial v_c} = \\sum\\limits_{-m \\le j \\le m} {\\frac {\\partial {J(v_c, w_{t+j}, U)}} {\\partial v_c}}$$\n",
    "\n",
    "$$\\frac {\\partial J_{\\text skip-gram}} {\\partial v_w} = 0 \\text {, for all } w \\ne c $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
